{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOHxHE3iIn4NoF/AlC68oJH"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# Show, Attend and Tell: Neural Image Captioning with Visual Attention\n","# Implementation based on Xu et al. (2015)\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.nn.utils.rnn import pack_padded_sequence\n","import torchvision.models as models\n","import torchvision.transforms as transforms\n","from torch.utils.data import Dataset, DataLoader\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from PIL import Image\n","import json\n","import os\n","from collections import Counter\n","import pickle\n","\n","# Set device\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(f\"Using device: {device}\")\n","\n","# =============================================================================\n","# 1. DATA PREPROCESSING AND VOCABULARY\n","# =============================================================================\n","\n","class Vocabulary:\n","    \"\"\"Vocabulary wrapper for image captions\"\"\"\n","    def __init__(self):\n","        self.word2idx = {}\n","        self.idx2word = {}\n","        self.idx = 0\n","\n","    def add_word(self, word):\n","        if not word in self.word2idx:\n","            self.word2idx[word] = self.idx\n","            self.idx2word[self.idx] = word\n","            self.idx += 1\n","\n","    def __call__(self, word):\n","        if not word in self.word2idx:\n","            return self.word2idx['<unk>']\n","        return self.word2idx[word]\n","\n","    def __len__(self):\n","        return len(self.word2idx)\n","\n","def build_vocab(caption_path, threshold=4):\n","    \"\"\"Build vocabulary from caption annotations\"\"\"\n","    # This is a simplified version - in practice you'd load from COCO annotations\n","    vocab = Vocabulary()\n","\n","    # Add special tokens\n","    vocab.add_word('<pad>')\n","    vocab.add_word('<start>')\n","    vocab.add_word('<end>')\n","    vocab.add_word('<unk>')\n","\n","    # In practice, you would:\n","    # 1. Load COCO annotations\n","    # 2. Count word frequencies\n","    # 3. Add words above threshold to vocabulary\n","\n","    # For demonstration, adding some common words\n","    common_words = ['a', 'the', 'man', 'woman', 'dog', 'cat', 'car', 'tree',\n","                   'house', 'red', 'blue', 'large', 'small', 'sitting', 'standing',\n","                   'walking', 'running', 'eating', 'playing', 'on', 'in', 'with']\n","\n","    for word in common_words:\n","        vocab.add_word(word)\n","\n","    return vocab\n","\n","# =============================================================================\n","# 2. CNN ENCODER (Feature Extractor)\n","# =============================================================================\n","\n","class EncoderCNN(nn.Module):\n","    \"\"\"CNN Encoder using ResNet as backbone\"\"\"\n","    def __init__(self, encoded_image_size=14):\n","        super(EncoderCNN, self).__init__()\n","        self.enc_image_size = encoded_image_size\n","\n","        # Load pretrained ResNet-101 and remove final layers\n","        resnet = models.resnet101(pretrained=True)\n","        modules = list(resnet.children())[:-2]  # Remove avgpool and fc layers\n","        self.resnet = nn.Sequential(*modules)\n","\n","        # Adaptive pooling to get fixed size output\n","        self.adaptive_pool = nn.AdaptiveAvgPool2d((encoded_image_size, encoded_image_size))\n","\n","        # Disable gradient computation for pretrained layers (optional)\n","        for param in self.resnet.parameters():\n","            param.requires_grad = False\n","\n","    def forward(self, images):\n","        \"\"\"\n","        Forward pass of encoder\n","        Args:\n","            images: (batch_size, 3, image_size, image_size)\n","        Returns:\n","            features: (batch_size, encoded_image_size, encoded_image_size, 2048)\n","        \"\"\"\n","        features = self.resnet(images)  # (batch_size, 2048, image_size/32, image_size/32)\n","        features = self.adaptive_pool(features)  # (batch_size, 2048, encoded_image_size, encoded_image_size)\n","        features = features.permute(0, 2, 3, 1)  # (batch_size, encoded_image_size, encoded_image_size, 2048)\n","        return features\n","\n","# =============================================================================\n","# 3. ATTENTION MECHANISM\n","# =============================================================================\n","\n","class Attention(nn.Module):\n","    \"\"\"Soft Attention mechanism\"\"\"\n","    def __init__(self, encoder_dim, decoder_dim, attention_dim):\n","        super(Attention, self).__init__()\n","        self.encoder_att = nn.Linear(encoder_dim, attention_dim)  # Linear layer to transform encoded image\n","        self.decoder_att = nn.Linear(decoder_dim, attention_dim)  # Linear layer to transform decoder's output\n","        self.full_att = nn.Linear(attention_dim, 1)  # Linear layer to calculate values to be softmax-ed\n","        self.relu = nn.ReLU()\n","        self.softmax = nn.Softmax(dim=1)  # Softmax layer to calculate weights\n","\n","    def forward(self, encoder_out, decoder_hidden):\n","        \"\"\"\n","        Forward pass of attention\n","        Args:\n","            encoder_out: (batch_size, num_pixels, encoder_dim)\n","            decoder_hidden: (batch_size, decoder_dim)\n","        Returns:\n","            attention_weighted_encoding: (batch_size, encoder_dim)\n","            alpha: (batch_size, num_pixels)\n","        \"\"\"\n","        att1 = self.encoder_att(encoder_out)  # (batch_size, num_pixels, attention_dim)\n","        att2 = self.decoder_att(decoder_hidden)  # (batch_size, attention_dim)\n","        att = self.full_att(self.relu(att1 + att2.unsqueeze(1))).squeeze(2)  # (batch_size, num_pixels)\n","        alpha = self.softmax(att)  # (batch_size, num_pixels)\n","        attention_weighted_encoding = (encoder_out * alpha.unsqueeze(2)).sum(dim=1)  # (batch_size, encoder_dim)\n","\n","        return attention_weighted_encoding, alpha\n","\n","# =============================================================================\n","# 4. LSTM DECODER WITH ATTENTION\n","# =============================================================================\n","\n","class DecoderWithAttention(nn.Module):\n","    \"\"\"LSTM Decoder with Attention\"\"\"\n","    def __init__(self, attention_dim, embed_dim, decoder_dim, vocab_size, encoder_dim=2048, dropout=0.5):\n","        super(DecoderWithAttention, self).__init__()\n","\n","        self.encoder_dim = encoder_dim\n","        self.attention_dim = attention_dim\n","        self.embed_dim = embed_dim\n","        self.decoder_dim = decoder_dim\n","        self.vocab_size = vocab_size\n","        self.dropout = dropout\n","\n","        # Attention network\n","        self.attention = Attention(encoder_dim, decoder_dim, attention_dim)\n","\n","        # Embedding layer\n","        self.embedding = nn.Embedding(vocab_size, embed_dim)\n","        self.dropout_layer = nn.Dropout(p=self.dropout)\n","\n","        # LSTM cell\n","        self.decode_step = nn.LSTMCell(embed_dim + encoder_dim, decoder_dim, bias=True)\n","\n","        # Linear layers for initial hidden and cell states\n","        self.init_h = nn.Linear(encoder_dim, decoder_dim)\n","        self.init_c = nn.Linear(encoder_dim, decoder_dim)\n","\n","        # Linear layer to create a sigmoid-activated gate\n","        self.f_beta = nn.Linear(decoder_dim, encoder_dim)\n","        self.sigmoid = nn.Sigmoid()\n","\n","        # Linear layer to find scores over vocabulary\n","        self.fc = nn.Linear(decoder_dim, vocab_size)\n","\n","        # Initialize some layers with the uniform distribution\n","        self.init_weights()\n","\n","    def init_weights(self):\n","        \"\"\"Initialize some parameters with values from the uniform distribution\"\"\"\n","        self.embedding.weight.data.uniform_(-0.1, 0.1)\n","        self.fc.bias.data.fill_(0)\n","        self.fc.weight.data.uniform_(-0.1, 0.1)\n","\n","    def load_pretrained_embeddings(self, embeddings):\n","        \"\"\"Load pretrained embeddings\"\"\"\n","        self.embedding.weight = nn.Parameter(embeddings)\n","\n","    def fine_tune_embeddings(self, fine_tune=True):\n","        \"\"\"Fine-tune embedding layer\"\"\"\n","        for p in self.embedding.parameters():\n","            p.requires_grad = fine_tune\n","\n","    def init_hidden_state(self, encoder_out):\n","        \"\"\"\n","        Creates the initial hidden and cell states for the decoder's LSTM\n","        Args:\n","            encoder_out: (batch_size, num_pixels, encoder_dim)\n","        Returns:\n","            h: (batch_size, decoder_dim)\n","            c: (batch_size, decoder_dim)\n","        \"\"\"\n","        mean_encoder_out = encoder_out.mean(dim=1)\n","        h = self.init_h(mean_encoder_out)  # (batch_size, decoder_dim)\n","        c = self.init_c(mean_encoder_out)  # (batch_size, decoder_dim)\n","        return h, c\n","\n","    def forward(self, encoder_out, encoded_captions, caption_lengths):\n","        \"\"\"\n","        Forward pass of decoder\n","        Args:\n","            encoder_out: (batch_size, enc_image_size, enc_image_size, encoder_dim)\n","            encoded_captions: (batch_size, max_caption_length)\n","            caption_lengths: (batch_size, 1)\n","        Returns:\n","            predictions: (batch_size, max_caption_length, vocab_size)\n","            alphas: (batch_size, max_caption_length, num_pixels)\n","        \"\"\"\n","        batch_size = encoder_out.size(0)\n","        encoder_dim = encoder_out.size(-1)\n","        vocab_size = self.vocab_size\n","\n","        # Flatten image\n","        encoder_out = encoder_out.view(batch_size, -1, encoder_dim)  # (batch_size, num_pixels, encoder_dim)\n","        num_pixels = encoder_out.size(1)\n","\n","        # Sort input data by decreasing lengths\n","        caption_lengths, sort_ind = caption_lengths.squeeze(1).sort(dim=0, descending=True)\n","        encoder_out = encoder_out[sort_ind]\n","        encoded_captions = encoded_captions[sort_ind]\n","\n","        # Embedding\n","        embeddings = self.embedding(encoded_captions)  # (batch_size, max_caption_length, embed_dim)\n","\n","        # Initialize LSTM state\n","        h, c = self.init_hidden_state(encoder_out)  # (batch_size, decoder_dim)\n","\n","        # We won't decode at the <end> position, since we've finished generating as soon as we generate <end>\n","        decode_lengths = (caption_lengths - 1).tolist()\n","\n","        # Create tensors to hold word prediction scores and alphas\n","        predictions = torch.zeros(batch_size, max(decode_lengths), vocab_size).to(device)\n","        alphas = torch.zeros(batch_size, max(decode_lengths), num_pixels).to(device)\n","\n","        # At each time-step, decode by attention-weighing the encoder's output based on the decoder's previous hidden state output\n","        # then generate a new word in the decoder with the previous word and the attention weighted encoding\n","        for t in range(max(decode_lengths)):\n","            batch_size_t = sum([l > t for l in decode_lengths])\n","            attention_weighted_encoding, alpha = self.attention(encoder_out[:batch_size_t], h[:batch_size_t])\n","            gate = self.sigmoid(self.f_beta(h[:batch_size_t]))  # gating scalar, (batch_size_t, encoder_dim)\n","            attention_weighted_encoding = gate * attention_weighted_encoding\n","            h, c = self.decode_step(\n","                torch.cat([embeddings[:batch_size_t, t, :], attention_weighted_encoding], dim=1),\n","                (h[:batch_size_t], c[:batch_size_t]))  # (batch_size_t, decoder_dim)\n","            preds = self.fc(self.dropout_layer(h))  # (batch_size_t, vocab_size)\n","            predictions[:batch_size_t, t, :] = preds\n","            alphas[:batch_size_t, t, :] = alpha\n","\n","        return predictions, alphas, encoded_captions, decode_lengths, sort_ind\n","\n","# =============================================================================\n","# 5. COMPLETE MODEL\n","# =============================================================================\n","\n","class ShowAttendTell(nn.Module):\n","    \"\"\"Complete Show, Attend and Tell model\"\"\"\n","    def __init__(self, vocab_size, attention_dim=512, embed_dim=512, decoder_dim=512, dropout=0.5):\n","        super(ShowAttendTell, self).__init__()\n","\n","        self.encoder = EncoderCNN()\n","        self.decoder = DecoderWithAttention(\n","            attention_dim=attention_dim,\n","            embed_dim=embed_dim,\n","            decoder_dim=decoder_dim,\n","            vocab_size=vocab_size,\n","            dropout=dropout\n","        )\n","\n","    def forward(self, images, captions, caption_lengths):\n","        \"\"\"Forward pass of complete model\"\"\"\n","        encoder_out = self.encoder(images)\n","        predictions, alphas, encoded_captions, decode_lengths, sort_ind = self.decoder(\n","            encoder_out, captions, caption_lengths\n","        )\n","        return predictions, alphas, encoded_captions, decode_lengths, sort_ind\n","\n","# =============================================================================\n","# 6. DATASET CLASS\n","# =============================================================================\n","\n","class CaptionDataset(Dataset):\n","    \"\"\"Custom Dataset for loading image-caption pairs\"\"\"\n","    def __init__(self, data_folder, data_name, split, transform=None):\n","        self.split = split\n","        assert self.split in {'TRAIN', 'VAL', 'TEST'}\n","\n","        # Open hdf5 file where images are stored\n","        self.h = h5py.File(os.path.join(data_folder, self.split + '_IMAGES_' + data_name + '.hdf5'), 'r')\n","        self.imgs = self.h['images']\n","\n","        # Captions per image\n","        self.cpi = self.h.attrs['captions_per_image']\n","\n","        # Load encoded captions\n","        with open(os.path.join(data_folder, self.split + '_CAPTIONS_' + data_name + '.json'), 'r') as j:\n","            self.captions = json.load(j)\n","\n","        # Load caption lengths\n","        with open(os.path.join(data_folder, self.split + '_CAPLENS_' + data_name + '.json'), 'r') as j:\n","            self.caplens = json.load(j)\n","\n","        # Transform\n","        self.transform = transform\n","\n","        # Total number of datapoints\n","        self.dataset_size = len(self.captions)\n","\n","    def __getitem__(self, i):\n","        # The Nth caption corresponds to the (N // captions_per_image)th image\n","        img = torch.FloatTensor(self.imgs[i // self.cpi] / 255.)\n","        if self.transform is not None:\n","            img = self.transform(img)\n","\n","        caption = torch.LongTensor(self.captions[i])\n","        caplen = torch.LongTensor([self.caplens[i]])\n","\n","        if self.split == 'TRAIN':\n","            return img, caption, caplen\n","        else:\n","            # For validation and testing, also return all 'captions_per_image' captions for evaluation\n","            all_captions = torch.LongTensor(\n","                self.captions[((i // self.cpi) * self.cpi):(((i // self.cpi) * self.cpi) + self.cpi)])\n","            return img, caption, caplen, all_captions\n","\n","    def __len__(self):\n","        return self.dataset_size\n","\n","# =============================================================================\n","# 7. TRAINING UTILITIES\n","# =============================================================================\n","\n","def clip_gradient(optimizer, grad_clip):\n","    \"\"\"Clips gradients computed during backpropagation to avoid explosion of gradients\"\"\"\n","    for group in optimizer.param_groups:\n","        for param in group['params']:\n","            if param.grad is not None:\n","                param.grad.data.clamp_(-grad_clip, grad_clip)\n","\n","def save_checkpoint(data_name, epoch, epochs_since_improvement, encoder, decoder,\n","                   encoder_optimizer, decoder_optimizer, bleu4, is_best):\n","    \"\"\"Saves model checkpoint\"\"\"\n","    state = {'epoch': epoch,\n","             'epochs_since_improvement': epochs_since_improvement,\n","             'bleu-4': bleu4,\n","             'encoder': encoder,\n","             'decoder': decoder,\n","             'encoder_optimizer': encoder_optimizer,\n","             'decoder_optimizer': decoder_optimizer}\n","    filename = 'checkpoint_' + data_name + '.pth.tar'\n","    torch.save(state, filename)\n","    if is_best:\n","        torch.save(state, 'BEST_' + filename)\n","\n","class AverageMeter(object):\n","    \"\"\"Keeps track of most recent, average, sum, and count of a metric\"\"\"\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count\n","\n","def accuracy(scores, targets, k):\n","    \"\"\"Computes top-k accuracy\"\"\"\n","    batch_size = targets.size(0)\n","    _, ind = scores.topk(k, 1, True, True)\n","    correct = ind.eq(targets.view(-1, 1).expand_as(ind))\n","    correct_total = correct.view(-1).float().sum()  # 0D tensor\n","    return correct_total.item() * (100.0 / batch_size)\n","\n","# =============================================================================\n","# 8. TRAINING FUNCTION\n","# =============================================================================\n","\n","def train(train_loader, encoder, decoder, criterion, encoder_optimizer, decoder_optimizer,\n","          epoch, vocab_size, grad_clip, alpha_c=1.0, print_freq=100):\n","    \"\"\"\n","    Performs one epoch's training\n","    \"\"\"\n","    decoder.train()  # train mode (dropout and batchnorm is used)\n","    encoder.train()\n","\n","    batch_time = AverageMeter()  # forward prop. + back prop. time\n","    data_time = AverageMeter()  # data loading time\n","    losses = AverageMeter()  # loss (per word decoded)\n","    top5accs = AverageMeter()  # top5 accuracy\n","\n","    start = time.time()\n","\n","    # Batches\n","    for i, (imgs, caps, caplens) in enumerate(train_loader):\n","        data_time.update(time.time() - start)\n","\n","        # Move to GPU, if available\n","        imgs = imgs.to(device)\n","        caps = caps.to(device)\n","        caplens = caplens.to(device)\n","\n","        # Forward prop.\n","        imgs = encoder(imgs)\n","        scores, caps_sorted, decode_lengths, alphas, sort_ind = decoder(imgs, caps, caplens)\n","\n","        # Since we decoded starting with <start>, the targets are all words after <start>, up to <end>\n","        targets = caps_sorted[:, 1:]\n","\n","        # Remove timesteps that we didn't decode at, or are pads\n","        # pack_padded_sequence is an easy trick to do this\n","        scores, _ = pack_padded_sequence(scores, decode_lengths, batch_first=True)\n","        targets, _ = pack_padded_sequence(targets, decode_lengths, batch_first=True)\n","\n","        # Calculate loss\n","        loss = criterion(scores, targets)\n","\n","        # Add doubly stochastic attention regularization\n","        loss += alpha_c * ((1. - alphas.sum(dim=1)) ** 2).mean()\n","\n","        # Back prop.\n","        decoder_optimizer.zero_grad()\n","        if encoder_optimizer is not None:\n","            encoder_optimizer.zero_grad()\n","        loss.backward()\n","\n","        # Clip gradients\n","        if grad_clip is not None:\n","            clip_gradient(decoder_optimizer, grad_clip)\n","            if encoder_optimizer is not None:\n","                clip_gradient(encoder_optimizer, grad_clip)\n","\n","        # Update weights\n","        decoder_optimizer.step()\n","        if encoder_optimizer is not None:\n","            encoder_optimizer.step()\n","\n","        # Keep track of metrics\n","        top5 = accuracy(scores, targets, 5)\n","        losses.update(loss.item(), sum(decode_lengths))\n","        top5accs.update(top5, sum(decode_lengths))\n","        batch_time.update(time.time() - start)\n","\n","        start = time.time()\n","\n","        # Print status\n","        if i % print_freq == 0:\n","            print('Epoch: [{0}][{1}/{2}]\\t'\n","                  'Batch Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n","                  'Data Load Time {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n","                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n","                  'Top-5 Accuracy {top5.val:.3f} ({top5.avg:.3f})'.format(\n","                      epoch, i, len(train_loader), batch_time=batch_time,\n","                      data_time=data_time, loss=losses, top5=top5accs))\n","\n","# =============================================================================\n","# 9. INFERENCE AND BEAM SEARCH\n","# =============================================================================\n","\n","def beam_search(encoder, decoder, image, word_map, beam_size=3):\n","    \"\"\"\n","    Beam Search for generating captions\n","\n","    Args:\n","        encoder: encoder model\n","        decoder: decoder model\n","        image: image tensor\n","        word_map: word2ix mapping\n","        beam_size: number of beams\n","\n","    Returns:\n","        seq: caption sequence\n","        alphas: attention weights\n","    \"\"\"\n","    k = beam_size\n","    vocab_size = len(word_map)\n","\n","    # Encode\n","    encoder_out = encoder(image)  # (1, enc_image_size, enc_image_size, encoder_dim)\n","    enc_image_size = encoder_out.size(1)\n","    encoder_dim = encoder_out.size(3)\n","\n","    # Flatten encoding\n","    encoder_out = encoder_out.view(1, -1, encoder_dim)  # (1, num_pixels, encoder_dim)\n","    num_pixels = encoder_out.size(1)\n","\n","    # We'll treat the problem as having a batch size of k\n","    encoder_out = encoder_out.expand(k, num_pixels, encoder_dim)  # (k, num_pixels, encoder_dim)\n","\n","    # Tensor to store top k previous words at each step; now they're just <start>\n","    k_prev_words = torch.LongTensor([[word_map['<start>']]] * k).to(device)  # (k, 1)\n","\n","    # Tensor to store top k sequences; now they're just <start>\n","    seqs = k_prev_words  # (k, 1)\n","\n","    # Tensor to store top k sequences' scores; now they're just 0\n","    top_k_scores = torch.zeros(k, 1).to(device)  # (k, 1)\n","\n","    # Tensor to store top k sequences' alphas; now they're just 1s\n","    seqs_alpha = torch.ones(k, 1, enc_image_size, enc_image_size).to(device)  # (k, 1, enc_image_size, enc_image_size)\n","\n","    # Lists to store completed sequences, their alphas and scores\n","    complete_seqs = list()\n","    complete_seqs_alpha = list()\n","    complete_seqs_scores = list()\n","\n","    # Start decoding\n","    step = 1\n","    h, c = decoder.init_hidden_state(encoder_out)\n","\n","    # s is a number less than or equal to k, because sequences are removed from this process once they hit <end>\n","    while True:\n","\n","        embeddings = decoder.embedding(k_prev_words).squeeze(1)  # (s, embed_dim)\n","\n","        awe, alpha = decoder.attention(encoder_out, h)  # (s, encoder_dim), (s, num_pixels)\n","\n","        alpha = alpha.view(-1, enc_image_size, enc_image_size)  # (s, enc_image_size, enc_image_size)\n","\n","        gate = decoder.sigmoid(decoder.f_beta(h))  # gating scalar, (s, encoder_dim)\n","        awe = gate * awe\n","\n","        h, c = decoder.decode_step(torch.cat([embeddings, awe], dim=1), (h, c))  # (s, decoder_dim)\n","\n","        scores = decoder.fc(h)  # (s, vocab_size)\n","        scores = F.log_softmax(scores, dim=1)\n","\n","        # Add\n","        scores = top_k_scores.expand_as(scores) + scores  # (s, vocab_size)\n","\n","        # For the first step, all k points will have the same scores (since same k previous words, h, c)\n","        if step == 1:\n","            top_k_scores, top_k_words = scores[0].topk(k, 0, True, True)  # (k)\n","        else:\n","            # Unroll and find top scores, and their unrolled indices\n","            top_k_scores, top_k_words = scores.view(-1).topk(k, 0, True, True)  # (k)\n","\n","        # Convert unrolled indices to actual indices of scores\n","        prev_word_inds = top_k_words // vocab_size  # (k)\n","        next_word_inds = top_k_words % vocab_size  # (k)\n","\n","        # Add new words to sequences, alphas\n","        seqs = torch.cat([seqs[prev_word_inds], next_word_inds.unsqueeze(1)], dim=1)  # (k, step+1)\n","        seqs_alpha = torch.cat([seqs_alpha[prev_word_inds], alpha[prev_word_inds].unsqueeze(1)],\n","                               dim=1)  # (k, step+1, enc_image_size, enc_image_size)\n","\n","        # Which sequences are incomplete (didn't reach <end>)?\n","        incomplete_inds = [ind for ind, next_word in enumerate(next_word_inds) if\n","                           next_word != word_map['<end>']]\n","        complete_inds = list(set(range(len(next_word_inds))) - set(incomplete_inds))\n","\n","        # Set aside complete sequences\n","        if len(complete_inds) > 0:\n","            complete_seqs.extend(seqs[complete_inds].tolist())\n","            complete_seqs_alpha.extend(seqs_alpha[complete_inds].tolist())\n","            complete_seqs_scores.extend(top_k_scores[complete_inds])\n","        k -= len(complete_inds)  # reduce beam length accordingly\n","\n","        # Proceed with incomplete sequences\n","        if k == 0:\n","            break\n","        seqs = seqs[incomplete_inds]\n","        seqs_alpha = seqs_alpha[incomplete_inds]\n","        h = h[prev_word_inds[incomplete_inds]]\n","        c = c[prev_word_inds[incomplete_inds]]\n","        encoder_out = encoder_out[prev_word_inds[incomplete_inds]]\n","        top_k_scores = top_k_scores[incomplete_inds].unsqueeze(1)\n","        k_prev_words = next_word_inds[incomplete_inds].unsqueeze(1)\n","\n","        # Break if things have been going on too long\n","        if step > 50:\n","            break\n","        step += 1\n","\n","    i = complete_seqs_scores.index(max(complete_seqs_scores))\n","    seq = complete_seqs[i]\n","    alphas = complete_seqs_alpha[i]\n","\n","    return seq, alphas\n","\n","# =============================================================================\n","# 10. VISUALIZATION UTILITIES\n","# =============================================================================\n","\n","def visualize_attention(image_path, seq, alphas, vocab, smooth=True):\n","    \"\"\"\n","    Visualizes caption with attention over image\n","    \"\"\"\n","    image = Image.open(image_path)\n","    image = image.resize([14 * 24, 14 * 24], Image.LANCZOS)\n","\n","    words = [vocab.idx2word[ind] for ind in seq]\n","\n","    for t in range(len(words)):\n","        if t > 50:\n","            break\n","        plt.subplot(int(np.ceil(len(words) / 5.)), 5, t + 1)\n","\n","        plt.text(0, 1, '%s' % (words[t]), color='black', fontsize=12)\n","        plt.imshow(image)\n","        current_alpha = alphas[t, :]\n","        if smooth:\n","            alpha = skimage.transform.pyramid_expand(current_alpha.numpy(), upscale=24, sigma=8)\n","        else:\n","            alpha = skimage.transform.resize(current_alpha.numpy(), [14 * 24, 14 * 24])\n","        if t == 0:\n","            plt.imshow(alpha, alpha=0)\n","        else:\n","            plt.imshow(alpha, alpha=0.8)\n","        plt.set_cmap(cm.Greys_r)\n","        plt.axis('off')\n","    plt.show()\n","\n","# =============================================================================\n","# 11. EXAMPLE USAGE AND TRAINING SETUP\n","# =============================================================================\n","\n","def main():\n","    # Hyperparameters\n","    batch_size = 32\n","    workers = 4\n","    encoder_lr = 1e-4\n","    decoder_lr = 4e-4\n","    grad_clip = 5.0\n","    alpha_c = 1.0  # regularization parameter for 'doubly stochastic attention'\n","    best_bleu4 = 0.\n","    epochs = 120\n","    epochs_since_improvement = 0\n","\n","    # Build vocabulary (in practice, load from preprocessed files)\n","    vocab = build_vocab('path_to_captions', threshold=5)\n","    vocab_size = len(vocab)\n","\n","    # Initialize model\n","    model = ShowAttendTell(vocab_size=vocab_size).to(device)\n","\n","    # Loss function\n","    criterion = nn.CrossEntropyLoss().to(device)\n","\n","    # Optimizers\n","    encoder_optimizer = torch.optim.Adam(params=filter(lambda p: p.requires_grad, model.encoder.parameters()),\n","                                         lr=encoder_lr)\n","    decoder_optimizer = torch.optim.Adam(params=filter(lambda p: p.requires_grad, model.decoder.parameters()),\n","                                         lr=decoder_lr)\n","\n","    # Data loaders (you would need to implement the actual data loading)\n","    # train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=workers)\n","    # val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=workers)\n","\n","    print(\"Model initialized successfully!\")\n","    print(f\"Vocabulary size: {vocab_size}\")\n","    print(f\"Using device: {device}\")\n","\n","    # Example of how training would work:\n","    # for epoch in range(epochs):\n","    #     train(train_loader, model.encoder, model.decoder, criterion,\n","    #           encoder_optimizer, decoder_optimizer, epoch, vocab_size, grad_clip, alpha_c)\n","\n","    return model, vocab\n","\n","if __name__ == \"__main__\":\n","    model, vocab = main()\n","\n","# =============================================================================\n","# 12. EXAMPLE INFERENCE CODE\n","# =============================================================================\n","\n","def generate_caption_example():\n","    \"\"\"Example of how to generate captions for new images\"\"\"\n","\n","    # Load trained model (you would load from checkpoint)\n","    # checkpoint = torch.load('BEST_checkpoint_coco_5_cap_per_img_5_min_word_freq.pth.tar')\n","    # model = checkpoint['model']\n","    # vocab = checkpoint['vocab']\n","\n","    # For demonstration, create dummy model and vocab\n","    vocab = build_vocab('dummy_path')\n","    model = ShowAttendTell(vocab_size=len(vocab)).to(device)\n","    model.eval()\n","\n","    # Image preprocessing\n","    transform = transforms.Compose([\n","        transforms.Resize((256, 256)),\n","        transforms.CenterCrop(224),\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n","                           std=[0.229, 0.224, 0.225])\n","    ])\n","\n","    # Load and preprocess image (example path)\n","    # image_path = 'path/to/your/image.jpg'\n","    # image = Image.open(image_path).convert('RGB')\n","    # image = transform(image).unsqueeze(0).to(device)\n","\n","    # Generate caption using beam search\n","    # with torch.no_grad():\n","    #     seq, alphas = beam_search(model.encoder, model.decoder, image, vocab.word2idx)\n","\n","    # Convert sequence to words\n","    # words = [vocab.idx2word[ind] for ind in seq if ind not in {vocab.word2idx['<start>'],\n","    #                                                            vocab.word2idx['<end>'],\n","    #                                                            vocab.word2idx['<pad>']}]\n","    # caption = ' '.join(words)\n","    # print(f\"Generated Caption: {caption}\")\n","\n","    # Visualize attention\n","    # visualize_attention(image_path, seq, alphas, vocab)\n","\n","    print(\"Caption generation example prepared!\")\n","\n","# =============================================================================\n","# 13. EVALUATION METRICS (BLEU SCORE)\n","# =============================================================================\n","\n","def evaluate_bleu(model, data_loader, vocab):\n","    \"\"\"\n","    Evaluate model using BLEU-4 score\n","    \"\"\"\n","    model.eval()\n","\n","    references = list()  # references (true captions) for calculating BLEU-4 score\n","    hypotheses = list()  # hypotheses (predictions)\n","\n","    with torch.no_grad():\n","        for i, (imgs, caps, caplens, allcaps) in enumerate(data_loader):\n","\n","            imgs = imgs.to(device)\n","\n","            # Generate captions\n","            for img in imgs:\n","                seq, _ = beam_search(model.encoder, model.decoder, img.unsqueeze(0), vocab.word2idx)\n","\n","                # Convert to words\n","                words = [vocab.idx2word[ind] for ind in seq if ind not in {vocab.word2idx['<start>'],\n","                                                                         vocab.word2idx['<end>'],\n","                                                                         vocab.word2idx['<pad>']}]\n","                hypotheses.append(' '.join(words))\n","\n","            # References\n","            for j in range(allcaps.shape[0]):\n","                img_caps = allcaps[j].tolist()\n","                img_captions = list(\n","                    map(lambda c: [vocab.idx2word[ind] for ind in c if ind not in {vocab.word2idx['<start>'],\n","                                                                                  vocab.word2idx['<end>'],\n","                                                                                  vocab.word2idx['<pad>']}],\n","                        img_caps))  # remove <start> and pads\n","                references.append(img_captions)\n","\n","    # Calculate BLEU-4 scores\n","    # Note: You would need to install nltk and use nltk.translate.bleu_score\n","    # from nltk.translate.bleu_score import corpus_bleu\n","    # bleu4 = corpus_bleu(references, hypotheses)\n","\n","    return 0.0  # placeholder return\n","\n","# =============================================================================\n","# 14. DATA PREPROCESSING PIPELINE\n","# =============================================================================\n","\n","def preprocess_coco_data(coco_dir, output_dir, captions_per_image=5, min_word_freq=5, max_len=50):\n","    \"\"\"\n","    Preprocess COCO dataset for training\n","\n","    This function would:\n","    1. Load COCO annotations\n","    2. Build vocabulary\n","    3. Encode captions\n","    4. Resize and save images\n","    5. Create train/val/test splits\n","    \"\"\"\n","\n","    import json\n","    import h5py\n","    from collections import Counter\n","    from PIL import Image\n","    import numpy as np\n","\n","    # Load COCO annotations (you would need to download COCO dataset)\n","    # with open(os.path.join(coco_dir, 'annotations', 'captions_train2014.json'), 'r') as f:\n","    #     train_data = json.load(f)\n","\n","    # This is a simplified preprocessing pipeline\n","    # In practice, you would:\n","\n","    # 1. Extract image paths and captions\n","    # 2. Build vocabulary from captions\n","    # 3. Encode captions using vocabulary\n","    # 4. Store processed data in HDF5 files\n","\n","    print(\"Data preprocessing pipeline prepared!\")\n","    print(\"To use with actual COCO data, uncomment and modify the preprocessing code.\")\n","\n","# =============================================================================\n","# 15. HYPERPARAMETER TUNING UTILITIES\n","# =============================================================================\n","\n","def hyperparameter_search():\n","    \"\"\"\n","    Example hyperparameter search configuration\n","    \"\"\"\n","\n","    # Define hyperparameter ranges\n","    hyperparams = {\n","        'attention_dim': [256, 512, 1024],\n","        'embed_dim': [256, 512, 1024],\n","        'decoder_dim': [256, 512, 1024],\n","        'learning_rate': [1e-4, 5e-4, 1e-3],\n","        'batch_size': [16, 32, 64],\n","        'dropout': [0.3, 0.5, 0.7]\n","    }\n","\n","    # You could implement grid search or random search here\n","    print(\"Hyperparameter search configuration prepared!\")\n","\n","    return hyperparams\n","\n","# =============================================================================\n","# 16. MODEL ANALYSIS AND DEBUGGING\n","# =============================================================================\n","\n","def analyze_attention_patterns(model, image, caption, vocab):\n","    \"\"\"\n","    Analyze attention patterns for debugging\n","    \"\"\"\n","    model.eval()\n","\n","    with torch.no_grad():\n","        # Forward pass\n","        encoder_out = model.encoder(image.unsqueeze(0))\n","\n","        # Get attention weights for each time step\n","        # This would require modifying the forward pass to return attention weights\n","\n","        print(\"Attention analysis prepared!\")\n","\n","def count_parameters(model):\n","    \"\"\"Count total number of trainable parameters\"\"\"\n","    total_params = sum(p.numel() for p in model.parameters())\n","    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","    print(f\"Total parameters: {total_params:,}\")\n","    print(f\"Trainable parameters: {trainable_params:,}\")\n","\n","    return total_params, trainable_params\n","\n","# =============================================================================\n","# 17. EXPORT AND DEPLOYMENT UTILITIES\n","# =============================================================================\n","\n","def export_model_for_inference(model, vocab, save_path):\n","    \"\"\"\n","    Export model for production inference\n","    \"\"\"\n","    # Save model state dict and vocabulary\n","    torch.save({\n","        'model_state_dict': model.state_dict(),\n","        'vocab': vocab,\n","        'model_config': {\n","            'vocab_size': len(vocab),\n","            'attention_dim': 512,\n","            'embed_dim': 512,\n","            'decoder_dim': 512\n","        }\n","    }, save_path)\n","\n","    print(f\"Model exported to {save_path}\")\n","\n","def load_model_for_inference(model_path):\n","    \"\"\"\n","    Load model for inference\n","    \"\"\"\n","    checkpoint = torch.load(model_path, map_location=device)\n","\n","    # Reconstruct model\n","    config = checkpoint['model_config']\n","    model = ShowAttendTell(\n","        vocab_size=config['vocab_size'],\n","        attention_dim=config['attention_dim'],\n","        embed_dim=config['embed_dim'],\n","        decoder_dim=config['decoder_dim']\n","    ).to(device)\n","\n","    # Load weights\n","    model.load_state_dict(checkpoint['model_state_dict'])\n","    vocab = checkpoint['vocab']\n","\n","    model.eval()\n","    return model, vocab\n","\n","# =============================================================================\n","# 18. TESTING AND VALIDATION\n","# =============================================================================\n","\n","def test_model_components():\n","    \"\"\"\n","    Test individual model components\n","    \"\"\"\n","    print(\"Testing model components...\")\n","\n","    # Test vocabulary\n","    vocab = build_vocab('dummy_path')\n","    assert len(vocab) > 0, \"Vocabulary should not be empty\"\n","\n","    # Test encoder\n","    encoder = EncoderCNN().to(device)\n","    dummy_image = torch.randn(2, 3, 224, 224).to(device)\n","    encoder_out = encoder(dummy_image)\n","    expected_shape = (2, 14, 14, 2048)\n","    assert encoder_out.shape == expected_shape, f\"Expected {expected_shape}, got {encoder_out.shape}\"\n","\n","    # Test attention\n","    attention = Attention(2048, 512, 512).to(device)\n","    encoder_out_flat = encoder_out.view(2, -1, 2048)\n","    decoder_hidden = torch.randn(2, 512).to(device)\n","    att_out, alpha = attention(encoder_out_flat, decoder_hidden)\n","    assert att_out.shape == (2, 2048), f\"Attention output shape mismatch: {att_out.shape}\"\n","    assert alpha.shape == (2, 196), f\"Alpha shape mismatch: {alpha.shape}\"\n","\n","    # Test decoder\n","    decoder = DecoderWithAttention(512, 512, 512, len(vocab)).to(device)\n","    dummy_captions = torch.randint(0, len(vocab), (2, 20)).to(device)\n","    dummy_lengths = torch.tensor([15, 18]).unsqueeze(1).to(device)\n","\n","    # Test complete model\n","    model = ShowAttendTell(len(vocab)).to(device)\n","    predictions, alphas, _, _, _ = model(dummy_image, dummy_captions, dummy_lengths)\n","\n","    print(\"All component tests passed!\")\n","\n","    # Count parameters\n","    total, trainable = count_parameters(model)\n","\n","    return True\n","\n","# Run tests\n","if __name__ == \"__main__\":\n","    # Run component tests\n","    test_model_components()\n","\n","    # Initialize main components\n","    model, vocab = main()\n","\n","    # Prepare example inference\n","    generate_caption_example()\n","\n","    # Show hyperparameter options\n","    hyperparams = hyperparameter_search()\n","\n","    print(\"\\n\" + \"=\"*80)\n","    print(\"SHOW, ATTEND AND TELL - IMPLEMENTATION COMPLETE\")\n","    print(\"=\"*80)\n","    print(\"\\nNext steps:\")\n","    print(\"1. Download and preprocess COCO dataset\")\n","    print(\"2. Modify data loading paths in the code\")\n","    print(\"3. Run training with your dataset\")\n","    print(\"4. Evaluate model performance\")\n","    print(\"5. Generate captions for new images\")\n","    print(\"\\nImplementation includes:\")\n","    print(\"- Complete encoder-decoder architecture with attention\")\n","    print(\"- Beam search for caption generation\")\n","    print(\"- Training and evaluation utilities\")\n","    print(\"- Visualization tools for attention maps\")\n","    print(\"- Model export/import for deployment\")\n","    print(\"=\"*80)"],"metadata":{"id":"Posm284Vh5n_"},"execution_count":null,"outputs":[]}]}