{"cells":[{"cell_type":"markdown","source":["# Show, Attend and Tell\n","\n","Alumno: Phabel Antonio López Delgado\n","\n","Redes Neuronales para Secuencias"],"metadata":{"id":"ctfbFfBs4_hz"},"id":"ctfbFfBs4_hz"},{"cell_type":"code","source":["!pip install torch==2.3.1 torchvision==0.18.1 torchaudio==2.3.1"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fR0UEMGmhfVx","executionInfo":{"status":"ok","timestamp":1755884061427,"user_tz":360,"elapsed":13802,"user":{"displayName":"Phabel López","userId":"10068691291319952559"}},"outputId":"b18434ba-0026-431b-93b2-1c55b1a4bed9"},"id":"fR0UEMGmhfVx","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torch==2.3.1 in /usr/local/lib/python3.12/dist-packages (2.3.1)\n","Requirement already satisfied: torchvision==0.18.1 in /usr/local/lib/python3.12/dist-packages (0.18.1)\n","Requirement already satisfied: torchaudio==2.3.1 in /usr/local/lib/python3.12/dist-packages (2.3.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.3.1) (3.19.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.1) (4.14.1)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from torch==2.3.1) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.3.1) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.1) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.3.1) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.1) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.1) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.1) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.1) (8.9.2.26)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.1) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.1) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.1) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.1) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.1) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.1) (2.20.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.1) (12.1.105)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision==0.18.1) (2.0.2)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision==0.18.1) (11.3.0)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.12/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.1) (12.6.85)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.3.1) (3.0.2)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->torch==2.3.1) (1.3.0)\n"]}]},{"cell_type":"code","source":["!pip install torchtext"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lS7JenC1gDaq","executionInfo":{"status":"ok","timestamp":1755884077480,"user_tz":360,"elapsed":16042,"user":{"displayName":"Phabel López","userId":"10068691291319952559"}},"outputId":"1c68b28e-00c4-49a7-9ec6-a2a1a9611722"},"id":"lS7JenC1gDaq","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torchtext in /usr/local/lib/python3.12/dist-packages (0.18.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from torchtext) (4.67.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from torchtext) (2.32.4)\n","Requirement already satisfied: torch>=2.3.0 in /usr/local/lib/python3.12/dist-packages (from torchtext) (2.3.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchtext) (2.0.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext) (3.19.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext) (4.14.1)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext) (8.9.2.26)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext) (2.20.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext) (12.1.105)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.12/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.3.0->torchtext) (12.6.85)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->torchtext) (3.4.3)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->torchtext) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->torchtext) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->torchtext) (2025.8.3)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.3.0->torchtext) (3.0.2)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->torch>=2.3.0->torchtext) (1.3.0)\n"]}]},{"cell_type":"code","execution_count":null,"id":"e0e5efb5-05b9-43ce-a784-534efb5044f3","metadata":{"id":"e0e5efb5-05b9-43ce-a784-534efb5044f3","executionInfo":{"status":"ok","timestamp":1755884091309,"user_tz":360,"elapsed":13827,"user":{"displayName":"Phabel López","userId":"10068691291319952559"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"89fc9c70-d8d1-48f1-d8ac-ae26daa60cd7"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/torchtext/vocab/__init__.py:4: UserWarning: \n","/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n","Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n","  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n","/usr/local/lib/python3.12/dist-packages/torchtext/utils.py:4: UserWarning: \n","/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n","Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n","  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"]}],"source":["import os\n","import numpy as np\n","from typing import List, Dict, Tuple\n","from collections import defaultdict, Counter\n","\n","import torch\n","import torchtext\n","import torchtext.vocab\n","import torchvision\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","from torch.utils.data import DataLoader, random_split\n","\n","import torchvision.transforms as transforms"]},{"cell_type":"markdown","id":"d5507d87-62f9-4541-b5de-969baa6fd092","metadata":{"id":"d5507d87-62f9-4541-b5de-969baa6fd092"},"source":["# 1. Introducción"]},{"cell_type":"markdown","id":"5b512227-d2be-4bd5-ae18-4c5d1313a0f5","metadata":{"id":"5b512227-d2be-4bd5-ae18-4c5d1313a0f5"},"source":["Bienvenidos al cuaderno de Jupyter sobre su proyecto para el módulo 12 del curso, en este, nos enfocaremos en replicar el modelo utilizado en el artículo \"Show, Attend and Tell: Neural Image Caption Generation with Visual Attention\". Este artículo revolucionó la generación de descripciones para imágenes al introducir un mecanismo de atención que le permite al modelo enfocarse en diferentes partes de la imagen mientras genera una descripción de esta, para esto, en lugar de simplemente procesar la imagen completa y generar una descripción, el modelo tiene la capacidad de poner atención en diferentes regiones de la imagen, lo que le permite generar descripciones más precisas y detalladas. Te recomiendo leer el artículo antes de continuar con el proyecto, lo puedes encontrar [aquí](http://proceedings.mlr.press/v37/xuc15.pdf).\n","\n","La idea centras detrás del mecanismo de atención es que, en lugar de codificar toda la imagen en un solo vector de características y usarlo para generar una descripción, el modelo genera un conjunto de vectores de anotaciones \"annotation vectors\" para diferentes regiones de la imagen. Posteriormente, mientras se genera la descripción palabra por palabra y utilizando estos vectores de anotaciones, el modelo decide a que regiones de la imagen \"atender\".\n","\n","En este cuaderno, diseñarás los elementos que componen a este modelo en PyTorch, procesarás y contruiras los conjuntos de entrenamiento usados, generarás el ciclo de entrenamiento del modelo y generarás descripciones de imágenes con este. El cuaderno contiene una definición base de código de todas estas secciones; sin embargo, es tu trabajo completarlas y pasar las pruebas unitarias asociadas para verificar que tu código funciona correctamente.\n"]},{"cell_type":"markdown","id":"1ad67a56-60ef-472e-9f33-e9b7fef8efac","metadata":{"id":"1ad67a56-60ef-472e-9f33-e9b7fef8efac"},"source":["## Índice de Contenidos\n","1. Definición del Modelo en PyTorch\n","    - Encoder: Extracción de vectores de anotación.\n","    - Attention: Generación del vector de contexto.\n","    - LSTM: Implementación personalizada de la celda LSTM.\n","    - Decoder: Generación de palabras utilizando la celda LSTM.\n","    - SequenceToSequnce: Orquestación del proceso de generación de descripciones.\n","2. Creación de Datasets y Dataloaders\n","    - Lectura y procesamiento de descripciones de imágenes.\n","    - Generación del vocabulario.\n","    - Definición del Dataset \"Flickr8kDataset\".\n","    - División de conjuntos de datos y definición de dataloaders.\n","3. Fase de Entrenamiento\n","    - Definición de modelos, optimizadores y funciones de pérdida.\n","    - Configuración de metaparámetros del entrenamiento.\n","    - Función de entrenamiento y seguimiento del progreso.\n","4. Análisis de Resutados, Generación de Descripciones.\n","    - Visualización de descripciones generadas por el modelo.\n","\n","Espero que este cuaderno les brinde una comprensión clara y práctica del mecanismo de atención y cómo se puede utilizar para mejorar la generación de descripciones de imágenes. ¡Suerte!"]},{"cell_type":"markdown","id":"c649f0a8-e0f8-4a03-9200-3f955c254b2c","metadata":{"id":"c649f0a8-e0f8-4a03-9200-3f955c254b2c"},"source":["## Detalles del Modelo\n","### Módulo Encoder\n","El encoder actúa como la primera etapa en nuestro proceso de generación de descripciones y juega un papel crucial en la extracción de información relevante de las imágenes. En lugar de simplemente convertir toda la imagen en un único vector de características, el encoder utilizado aquí se basa en la arquitectura vgg16, una red neuronal convolucional profunda y ampliamente reconocida por su eficacia en tareas de visión por computadora. El objetivo principal del encoder es producir lo que llamamos \"vectores de anotaciones\". Estos vectores representan diferentes regiones de la imagen y capturan la esencia de lo que está presente en esas regiones. En términos prácticos, tomamos los feature maps de la última capa convolucional de vgg16 antes de aplicar max pooling. Estos feature maps, originalmente de dimensiones (14, 14, 512), se transforman para que tengan dimensiones (196, 512), lo que nos indica que tenemos $L=196$ distintos vectores de anotaciones para cada imagen, cada uno con $D=512$ valores. Este conjunto de vectores que denominaremos con la letra $a$ serán la base sobre la cual el mecanismo de atención decidirá a qué partes de la imagen \"atender\" mientras genera la descripción.\n","\n","$$a = \\left\\{ \\boldsymbol{a_1}, ..., \\boldsymbol{a_L}\\right\\}, \\;\\;\\boldsymbol{a_i} \\in \\mathbb{R}^D$$\n","## Módulo de Atención\n","El módulo de atención es una pieza central en nuestro modelo de generación de descripciones y es el responsable de decidir a qué partes específicas de la imagen debe \"atender\" el modelo en cada paso de la generación de la descripción. La idea es que, en lugar de usar toda la información de la imagen en cada paso, el modelo pondera diferentes regiones de la imagen basándose en su relevancia para la palabra que está generando.\n","\n","Para lograr esto, el módulo de atención toma como entrada el estado oculto anterior del decodificador $\\boldsymbol{s_{i-1}}$ y los vectores de anotaciones $a$ y los injecta en una red neuronal (que es entrenada con el resto del modelo) para calcular \"puntuaciones de atención\" $e_{ij}$ para cada vector de anotación. Estas puntuaciones indican que tan relevante es cada parte de la imagen para la generación de la siguiente palabra de la descripción de la imagen.\n","\n","A continuación, estas puntuaciones de atención $e_{ij}$ se pasan por una función softmax para obtener los pesos de atención \"alpha\" $\\alpha_{ij}$. La función softmax asegura que estos pesos estén en el rango $[0,1]$ y que su suma sea $1$, lo que permite que actúen como probabilidades que indican la importancia de cada vector de anotación en relación con los demás.\n","\n","Una vez calculadas las puntuaciones, se aplican a los vectores de anotación para producir un \"vector de contexto\" $\\boldsymbol{z_i}$. Este vector de contexto es una combinación ponderada de los vectores de anotación y captura las partes más relevantes de la imagen para el paso actual de generación de palabras. Es este vector de contexto el que se pasa al decodificador para influir en la generación de la siguiente palabra.\n","\n","$$\\boldsymbol{z_i} = \\sum_{j=1}^{T_x} \\alpha_{ij}\\boldsymbol{h_j}$$\n","\n","### Módulo LSTM\n","En nuestro modelo de generación de descripciones, utilizamos una variante especial de la celda LSTM definida en el artíclo. A diferencia de las celdas LSTM tradicionales, esta versión incorpora el vector de contexto, que proviene del módulo de atención y representa las regiones relevantes de la imagen. Esta integración del vector de contexto en la celda LSTM asegura que la información visual relevante se tenga en cuenta en cada paso de la generación de la descripción. Específicamente, el vector de contexto se inyecta en las compuertas de la LSTM, influenciando las decisiones sobre qué información retener, actualizar o ignorar en el estado de celda y el estado oculto. Las ecuaciones de la celda LSTM son las siguientes:\n","\n","\\begin{align*}\n","\\boldsymbol{i_t} &= \\sigma(\\boldsymbol{W_i} \\boldsymbol{E_{y_{t-1}}} + \\boldsymbol{U_i}\\boldsymbol{h_{t-1}} + \\boldsymbol{Z_i}\\boldsymbol{z_i} + \\boldsymbol{b_i})\\\\\n","\\boldsymbol{f_t} &= \\sigma(\\boldsymbol{W_f} \\boldsymbol{E_{y_{t-1}}} + \\boldsymbol{U_f}\\boldsymbol{h_{t-1}} + \\boldsymbol{Z_f}\\boldsymbol{z_i} + \\boldsymbol{b_f})\\\\\n","\\boldsymbol{c_t} &= \\boldsymbol{f_t}\\boldsymbol{c_{t-1}} + \\boldsymbol{i_t}tanh\\left( \\boldsymbol{W_c} \\boldsymbol{E_{y_{t-1}}} + \\boldsymbol{U_c}\\boldsymbol{h_{t-1}} + \\boldsymbol{Z_c}\\boldsymbol{z_i} + \\boldsymbol{b_c} \\right)\\\\\n","\\boldsymbol{o_t} &= \\sigma(\\boldsymbol{W_o} \\boldsymbol{E_{y_{t-1}}} + \\boldsymbol{U_o}\\boldsymbol{h_{t-1}} + \\boldsymbol{Z_o}\\boldsymbol{z_i} + \\boldsymbol{b_o})\\\\\n","\\boldsymbol{h_t} &= \\boldsymbol{o_t}tanh(\\boldsymbol{c_t})\n","\\end{align*}\n","\n","\n","### Módulo Decoder\n","El módulo Decoder es el encargado de traducir los vectores de características de la imagen, junto con el contexto proporcionado por el módulo de atención, en una secuencia de palabras que forman la descripción de la imagen. Funciona en pasos, generando una palabra en cada iteración hasta completar la descripción.\n","\n","El proceso comienza con la codificación en embeddings de la palabra anteriormente generada (o la palabra inicial en el primer paso). Esta codificación convierte la palabra en un vector que puede ser procesado por la red. A continuación, este vector, junto con el vector de contexto y los estados oculto y de celda anteriores, se pasan a través de nuestra celda LSTM especializada. Una vez que se obtiene el nuevo estado oculto de la LSTM, este se pasa a través de una capa lineal para obtener los \"logits\" para cada palabra en el vocabulario. Estos \"logits\" representan las puntaciones de cada palabra de ser la siguiente en la descripción. La palabra con el valor más alto se selecciona como la siguiente palabra en la secuencia.\n","\n","### Módulo SequenceToSequence\n","El módulo SequenceToSequence es el corazón del proceso de generación de descripciones y actúa como un orquestador que une todos los componentes principales del modelo: el Encoder, el módulo de Atención y el Decoder.\n","\n","Codificación de la Imagen: Primero, las imágenes se pasan a través del Encoder, que extrae características visuales relevantes de la imagen. Estas características, también conocidas como vectores danotacionesón, actúan como una representación comprimida de la imagen y capturan la información esencial necesaria para generar una descripción.\n","\n","Iniciación de la Decodificación: Antes de comenzar el proceso de decodificación, se inicializan los estados oculto y de celda del Decoder con ceros. Además, se establece la primera palabra de la secuencia de salida como el token <SOS> (Start of Sequence), indicando el comienzo de una nueva descripción.\n","\n","Decodificación Paso a Paso: A continuación, en cada paso temporal, el modelo utiliza el estado oculto actual y los vectoranotacionestación para calcular un vector de contexto utilizando el módulo de Atención. Este vector de contexto, que es una representación ponderada de las características visuales, se combinlos embeddingstación de la palabra actual y se pasa al Decoder. El Decoder genera logits para la siguiente palabra en la descripción. Durante el entrenamiento, se utiliza la técnica de \"teacher forcing\", lo que significa que la palabra real del conjunto de datos se utiliza como entrada para el siguiente paso, en lugar de la palabra predicha en el paso anterior.\n","\n","Salida: El proceso de decodificación continúa hasta que se ha generado una secuencia completa de palabras. La salida final es un tensor que contiene logits para cada palabra en el vocabulario, en cada paso temporal."]},{"cell_type":"markdown","id":"f2bb0e9e-ba4e-4b79-839b-ca4c575432ce","metadata":{"id":"f2bb0e9e-ba4e-4b79-839b-ca4c575432ce"},"source":["# 2. Definición del Modelo"]},{"cell_type":"markdown","id":"16fe76fa-f97e-4941-bf2b-5f8a53e2af23","metadata":{"id":"16fe76fa-f97e-4941-bf2b-5f8a53e2af23"},"source":["En esta sección, implementarás los cuatro módulos esenciales para el modelo: Encoder, Attention, LSTM y SequenceToSequence. Para todos, utiliza las siguientes instrucciones:\n","\n","- Completar el Código: En cada módulo, encontrarán secciones marcadas con \"TODO\". Tu tarea es completar estas secciones con el código necesario para que este funcione correctamente según las especificaciones y la lógica descrita.\n","\n","- Pruebas Unitarias: Después de completar las secciones \"TODO\" en cada módulo, ejecuta las prueba unitarias correspondiente para ese módulo. Estas pruebas están diseñadas para ayudarles a verificar si su implementación es correcta. Es esencial que no modifiquen estas pruebas; si lo hacen, es posible que no les sirvan como guía adecuada.\n","\n","- Revisión del Código: Tengan en cuenta que, aunque las pruebas unitarias son una herramienta útil, pasar estas pruebas no garantiza una calificación completa. Su código será revisado detalladamente para asegurarse de que cumple con todas las especificaciones y que está correctamente implementado. Las pruebas están ahí para servir como una guía inicial, pero no son exhaustivas.\n","\n","Finalmente, recuerden revisar y probar su código cuidadosamente antes de enviarlo. Asegúrense de entender cada parte del código que escriben y cómo contribuye al funcionamiento general del modelo.\n","\n","¡Buena suerte y a programar!"]},{"cell_type":"markdown","id":"3b54cad7-c357-4e37-9c1e-ac0f9cef6300","metadata":{"id":"3b54cad7-c357-4e37-9c1e-ac0f9cef6300"},"source":["## 2.1 Módulo Encoder"]},{"cell_type":"code","source":["class Encoder(torch.nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","        # TODO: Importa el modelo vgg16 utilizando torchvision, carga los pesos por defecto.\n","        vgg16 = torchvision.models.vgg16(pretrained=True)\n","\n","        # TODO: Elimina la última capa de max pooling para obtener el mapa de características de 14x14x512.\n","        self.features = torch.nn.Sequential(*list(vgg16.features.children())[:-1])\n","\n","        # TODO: Congela las capas del modelo para evitar actualizar los pesos durante el entrenamiento.\n","        for param in self.features.parameters():\n","            param.requires_grad = False\n","\n","    def forward(self, x):\n","        # x: [batch_size, 3, 224, 224]\n","        # TODO: Pasar la entrada a través de las capas de características de VGG16. La salida debe de tener\n","        # dimensiones [batch_size, 512, 14, 14]\n","        x = self.features(x)\n","\n","        # Se redimensiona el x a [batch_size, 512, 196].\n","        x = x.permute(0, 2, 3, 1)  # [batch_size, 14, 14, 512]\n","        x = x.view(x.size(0), -1, x.size(3))  # [batch_size, 196, 512]\n","\n","        return x"],"metadata":{"id":"08LqE_ZRf8eW"},"id":"08LqE_ZRf8eW","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"833191ba-3f61-4945-b04e-a440f36b4fff","metadata":{"id":"833191ba-3f61-4945-b04e-a440f36b4fff","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1755884110996,"user_tz":360,"elapsed":19688,"user":{"displayName":"Phabel López","userId":"10068691291319952559"}},"outputId":"ac3d442e-6e64-4282-97a3-d7444de26209"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]},{"output_type":"stream","name":"stdout","text":["¡Todas las pruebas pasaron con éxito!\n"]}],"source":["def test_encoder():\n","    # Crear un tensor aleatorio con forma [batch_size, 3, 224, 224]\n","    input_tensor = torch.rand((8, 3, 224, 224))\n","\n","    # Inicializar el encoder\n","    encoder = Encoder()\n","\n","    # Pasar el tensor a través del encoder\n","    output = encoder(input_tensor)\n","\n","    # Prueba 1: Verificar las dimensiones de salida\n","    assert output.shape == (8, 196, 512), f\"Se esperaba una forma de salida (8, 196, 512), pero se obtuvo {output.shape}\"\n","\n","    # Prueba 2: Verificar si las capas de VGG16 están congeladas\n","    for param in encoder.features.parameters():\n","        assert not param.requires_grad, \"¡Algunas capas de VGG16 no están congeladas!\"\n","\n","    print(\"¡Todas las pruebas pasaron con éxito!\")\n","\n","test_encoder()"]},{"cell_type":"markdown","id":"61d41701-bc79-4fa2-8608-dd5b258f4c14","metadata":{"id":"61d41701-bc79-4fa2-8608-dd5b258f4c14"},"source":["## 2.2 Módulo de Atención"]},{"cell_type":"code","source":["class Attention(torch.nn.Module):\n","    def __init__(self, encoder_dim, decoder_dim, attention_dim):\n","        super().__init__()\n","\n","        # Capas lineales necesarias para calcular las puntuaciones de atención\n","        self.U_a = torch.nn.Linear(encoder_dim, attention_dim)  # [encoder_dim, attention_dim]\n","        self.W_a = torch.nn.Linear(decoder_dim, attention_dim)  # [decoder_dim, attention_dim]\n","        self.v_a = torch.nn.Linear(attention_dim, 1)            # [attention_dim, 1]\n","\n","    def forward(self, h, encoder_out):\n","        \"\"\"\n","        h: previous hidden state of the decoder, shape: [batch_size, decoder_dim]\n","        encoder_out: output of the encoder, shape: [batch_size, L, encoder_dim]\n","        \"\"\"\n","        # TODO: Aplica la capa lineal a los vectores de anotaciones. El resultado se espera que tenga las\n","        # siguientes dimensiones [batch_size, L, attention_dim]\n","        u_hs = self.U_a(encoder_out)\n","\n","        # TODO: Procesa el estado oculto del decodificador con su capa lineal y expande sus dimensiones para\n","        # que coincida (en número de dimensiones) con la del resultado anterior.\n","        w_ah = self.W_a(h)                    # [batch_size, attention_dim]\n","        w_ah = w_ah.unsqueeze(1)              # [batch_size, 1, attention_dim]\n","\n","        # Se calculan las puntuaciones de atención.\n","        # F.tanh(u_hs + w_ah): [batch_size, L, attention_dim]\n","        attn_scores = self.v_a(torch.nn.functional.tanh(u_hs + w_ah))  # [batch_size, L, 1]\n","        attn_scores = attn_scores.squeeze(2)  # [batch_size, L]\n","\n","        # TODO: Calcula los pesos de atención usando softmax sobre attn_scores.\n","        alpha = torch.nn.functional.softmax(attn_scores, dim=1)  # [batch_size, L]\n","\n","        # Se calcula el vector de contexto como la suma ponderada de las salidas del encoder.\n","        context_vector = (encoder_out * alpha.unsqueeze(2)).sum(dim=1)  # [batch_size, encoder_dim]\n","\n","        return context_vector, alpha"],"metadata":{"id":"mvxGY5_7gMI7"},"id":"mvxGY5_7gMI7","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"74bf20dc-353d-4328-af92-170ab796f2e7","metadata":{"id":"74bf20dc-353d-4328-af92-170ab796f2e7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1755884111066,"user_tz":360,"elapsed":43,"user":{"displayName":"Phabel López","userId":"10068691291319952559"}},"outputId":"b9cd1f1d-8b1d-4424-980e-7bd8e284ffd1"},"outputs":[{"output_type":"stream","name":"stdout","text":["¡Todas las pruebas pasaron con éxito!\n"]}],"source":["def test_attention_module():\n","    # Parámetros de prueba\n","    batch_size = 8\n","    encoder_dim = 512\n","    decoder_dim = 256\n","    attention_dim = 128\n","\n","    # Crear tensores aleatorios para la salida del encoder y el estado oculto del decodificador\n","    encoder_out = torch.rand((batch_size, 196, encoder_dim))\n","    h = torch.rand((batch_size, decoder_dim))\n","\n","    # Inicializar el módulo de atención\n","    attention = Attention(encoder_dim, decoder_dim, attention_dim)\n","\n","    # Obtener el vector de contexto y los pesos de atención\n","    context_vector, alpha = attention(h, encoder_out)\n","\n","    # Prueba 1: Verificar las dimensiones de salida\n","    assert context_vector.shape == (batch_size, encoder_dim), f\"Se esperaba una forma de salida (batch_size, encoder_dim), pero se obtuvo {context_vector.shape}\"\n","    assert alpha.shape == (batch_size, 196), f\"Se esperaba una forma de salida (batch_size, 196) para los pesos de atención, pero se obtuvo {alpha.shape}\"\n","\n","    # Prueba 2: Verificar que la suma de los pesos de atención sea 1\n","    assert torch.allclose(alpha.sum(dim=1), torch.tensor([1.0] * batch_size), atol=1e-5), \"La suma de los pesos de atención no es 1 para todos los ejemplos en el batch.\"\n","\n","    print(\"¡Todas las pruebas pasaron con éxito!\")\n","\n","test_attention_module()"]},{"cell_type":"markdown","id":"6b905a89-6fe3-479d-bf24-43e55011ff72","metadata":{"id":"6b905a89-6fe3-479d-bf24-43e55011ff72"},"source":["## 2.2 Módulo Celda LSTM"]},{"cell_type":"code","source":["class LSTM(torch.nn.Module):\n","    def __init__(self, input_size, hidden_size, context_size):\n","        super().__init__()\n","\n","        # Se definen los parámetros para la compuerta de entrada (input gate).\n","        self.W_i = torch.nn.Parameter(torch.randn(input_size, hidden_size))\n","        self.U_i = torch.nn.Parameter(torch.randn(hidden_size, hidden_size))\n","        self.Z_i = torch.nn.Parameter(torch.randn(context_size, hidden_size))\n","        self.b_i = torch.nn.Parameter(torch.randn(hidden_size))\n","\n","        # TODO: Define los parámetros para la compuerta de olvido (Forget gate).\n","        self.W_f = torch.nn.Parameter(torch.randn(input_size, hidden_size))\n","        self.U_f = torch.nn.Parameter(torch.randn(hidden_size, hidden_size))\n","        self.Z_f = torch.nn.Parameter(torch.randn(context_size, hidden_size))\n","        self.b_f = torch.nn.Parameter(torch.randn(hidden_size))\n","\n","        # TODO: Define los parámetros para la compuerta de celda (Cell gate).\n","        self.W_c = torch.nn.Parameter(torch.randn(input_size, hidden_size))\n","        self.U_c = torch.nn.Parameter(torch.randn(hidden_size, hidden_size))\n","        self.Z_c = torch.nn.Parameter(torch.randn(context_size, hidden_size))\n","        self.b_c = torch.nn.Parameter(torch.randn(hidden_size))\n","\n","        # TODO: Define los parámetros para la compuerta de salida (Output gate).\n","        self.W_o = torch.nn.Parameter(torch.randn(input_size, hidden_size))\n","        self.U_o = torch.nn.Parameter(torch.randn(hidden_size, hidden_size))\n","        self.Z_o = torch.nn.Parameter(torch.randn(context_size, hidden_size))\n","        self.b_o = torch.nn.Parameter(torch.randn(hidden_size))\n","\n","    def forward(self, x, hc, context):\n","        h, c = hc\n","\n","        # Se calcula la activación de la compuerta de entrada.\n","        i = torch.sigmoid(x @ self.W_i + h @ self.U_i + context @ self.Z_i + self.b_i)\n","\n","        # TODO: Calcula la activación de la compuerta de olvido\n","        f = torch.sigmoid(x @ self.W_f + h @ self.U_f + context @ self.Z_f + self.b_f)\n","\n","        # Se calcula candidato de activación para la compuerta de celda.\n","        c_tilde = torch.tanh(x @ self.W_c + h @ self.U_c + context @ self.Z_c + self.b_c)\n","\n","        # TODO: Calcula la activación de la compuerta de salida\n","        o = torch.sigmoid(x @ self.W_o + h @ self.U_o + context @ self.Z_o + self.b_o)\n","\n","        # TODO: Actualiza el estado de celda usando las activaciones anteriores\n","        c = f * c + i * c_tilde\n","\n","        # TODO: Calcula el nuevo estado oculto\n","        h = o * torch.tanh(c)\n","\n","        return h, c  # [batch_size, hidden_dim], [batch_size, hidden_dim]"],"metadata":{"id":"p6Ii0vv6gwEt"},"id":"p6Ii0vv6gwEt","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"35cef3bf-a6c2-458e-919e-16c3cb1ffd4d","metadata":{"id":"35cef3bf-a6c2-458e-919e-16c3cb1ffd4d","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1755884111233,"user_tz":360,"elapsed":107,"user":{"displayName":"Phabel López","userId":"10068691291319952559"}},"outputId":"2f693cac-edc5-4606-ec7c-568118db163e"},"outputs":[{"output_type":"stream","name":"stdout","text":["¡Todas las pruebas pasaron con éxito!\n"]}],"source":["def test_lstm_module():\n","    # Parámetros de prueba\n","    batch_size = 8\n","    input_size = 256\n","    hidden_size = 512\n","    context_size = 512\n","\n","    # Crear tensores aleatorios para la entrada, estado oculto, estado de celda y vector de contexto\n","    x = torch.rand((batch_size, input_size))\n","    h = torch.rand((batch_size, hidden_size))\n","    c = torch.rand((batch_size, hidden_size))\n","    context = torch.rand((batch_size, context_size))\n","\n","    # Inicializar el módulo LSTM\n","    lstm = LSTM(input_size, hidden_size, context_size)\n","\n","    # Obtener el estado oculto y estado de celda actualizados\n","    h_new, c_new = lstm(x, (h, c), context)\n","\n","    # Prueba 1: Verificar las dimensiones de salida\n","    assert h_new.shape == (batch_size, hidden_size), f\"Se esperaba una forma de salida (batch_size, hidden_size) para h, pero se obtuvo {h_new.shape}\"\n","    assert c_new.shape == (batch_size, hidden_size), f\"Se esperaba una forma de salida (batch_size, hidden_size) para c, pero se obtuvo {c_new.shape}\"\n","\n","    # Prueba 2: Verificar que los valores de salida no sean iguales a los de entrada (se ha realizado alguna actualización)\n","    assert not torch.equal(h, h_new), \"El estado oculto no se actualizó.\"\n","    assert not torch.equal(c, c_new), \"El estado de celda no se actualizó.\"\n","\n","    print(\"¡Todas las pruebas pasaron con éxito!\")\n","\n","test_lstm_module()"]},{"cell_type":"markdown","id":"72adfd1f-e99c-49ca-aaff-2e1ef0a4d011","metadata":{"id":"72adfd1f-e99c-49ca-aaff-2e1ef0a4d011"},"source":["## 2.3 Módulo Decoder"]},{"cell_type":"code","source":["class Decoder(torch.nn.Module):\n","    def __init__(self, embed_dim, decoder_dim, vocab_size, encoder_dim):\n","        super().__init__()\n","\n","        # TODO: Define la capa embedding para las palabras.\n","        self.embedding = torch.nn.Embedding(vocab_size, embed_dim)\n","\n","        # TODO: Inicializa la celda LSTM especializada.\n","        self.lstm = LSTM(embed_dim, decoder_dim, encoder_dim)\n","\n","        self.fc = torch.nn.Linear(decoder_dim, vocab_size)\n","\n","    def forward(self, context_vector, h, c, embeddings):\n","        \"\"\"Decode image feature vectors and generates captions.\n","        :param context_vector: attention-weighted encoding, a tensor of dimension (batch_size, encoder_dim)\n","        :param h: previous hidden state, a tensor of dimension (batch_size, decoder_dim)\n","        :param c: previous cell state, a tensor of dimension (batch_size, decoder_dim)\n","        :param embeddings: embedded word of the previous step, a tensor of dimension (batch_size, embed_dim)\n","        :return: predictions for next word, hidden state, cell state\n","        \"\"\"\n","        # TODO: Pasa los embeddings y el vector de contexto a través de la celda LSTM.\n","        hidden, cell = self.lstm(embeddings, (h, c), context_vector)\n","        # hidden: [batch_size, hidden_dim]\n","        # cell: [batch_size, hidden_dim]\n","\n","        # TODO: Transforma el estado oculto en \"logits\" para cada palabra del vocabulario\n","        logits = self.fc(hidden)\n","        # logits: [batch_size, vocab_size]\n","\n","        return logits, hidden, cell"],"metadata":{"id":"RIXQDlnOg-mu"},"id":"RIXQDlnOg-mu","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"db1c089c-0c83-4307-a42a-2b60cc6ea91c","metadata":{"id":"db1c089c-0c83-4307-a42a-2b60cc6ea91c","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1755884111273,"user_tz":360,"elapsed":40,"user":{"displayName":"Phabel López","userId":"10068691291319952559"}},"outputId":"91a1607a-8983-4131-df62-6d1e4cd4d750"},"outputs":[{"output_type":"stream","name":"stdout","text":["¡Todas las pruebas pasaron con éxito!\n"]}],"source":["def test_decoder_module():\n","    # Parámetros de prueba\n","    batch_size = 8\n","    embed_dim = 256\n","    decoder_dim = 512\n","    vocab_size = 1000\n","    encoder_dim = 512\n","\n","    # Crear tensores aleatorios para el vector de contexto, estado oculto, estado de celda y embeddings\n","    context_vector = torch.rand((batch_size, encoder_dim))\n","    h = torch.rand((batch_size, decoder_dim))\n","    c = torch.rand((batch_size, decoder_dim))\n","    embeddings = torch.rand((batch_size, embed_dim))\n","\n","    # Inicializar el módulo Decoder\n","    decoder = Decoder(embed_dim, decoder_dim, vocab_size, encoder_dim)\n","\n","    # Obtener los logits, estado oculto y estado de celda actualizados\n","    logits, h_new, c_new = decoder(context_vector, h, c, embeddings)\n","\n","    # Prueba 1: Verificar las dimensiones de salida\n","    assert logits.shape == (batch_size, vocab_size), f\"Se esperaba una forma de salida (batch_size, vocab_size) para logits, pero se obtuvo {logits.shape}\"\n","    assert h_new.shape == (batch_size, decoder_dim), f\"Se esperaba una forma de salida (batch_size, decoder_dim) para h, pero se obtuvo {h_new.shape}\"\n","    assert c_new.shape == (batch_size, decoder_dim), f\"Se esperaba una forma de salida (batch_size, decoder_dim) para c, pero se obtuvo {c_new.shape}\"\n","\n","    # Prueba 2: Verificar que los valores de salida no sean iguales a los de entrada (se ha realizado alguna actualización)\n","    assert not torch.equal(h, h_new), \"El estado oculto no se actualizó.\"\n","    assert not torch.equal(c, c_new), \"El estado de celda no se actualizó.\"\n","\n","    print(\"¡Todas las pruebas pasaron con éxito!\")\n","\n","test_decoder_module()"]},{"cell_type":"markdown","id":"a2bbca47-cfbb-466a-b666-8ce39a6c0e60","metadata":{"id":"a2bbca47-cfbb-466a-b666-8ce39a6c0e60"},"source":["## 2.4 Módulo SequenceToSequence"]},{"cell_type":"code","source":["class SequenceToSequence(torch.nn.Module):\n","    def __init__(self, embed_dim, decoder_dim, vocab_size, encoder_dim, attention_dim, device, vocab):\n","        super().__init__()\n","\n","        # Definición de los módulos principales: encoder, attention y decoder.\n","        self._encoder = Encoder()\n","        self._attention = Attention(encoder_dim, decoder_dim, attention_dim)\n","        self._decoder = Decoder(embed_dim, decoder_dim, vocab_size, encoder_dim)\n","        self._embedding = torch.nn.Embedding(vocab_size, embed_dim)\n","        self._device = device\n","        self._vocab = vocab\n","\n","        # Store decoder_dim as instance variable\n","        self._decoder_dim = decoder_dim\n","        self._vocab_size = vocab_size\n","\n","    def forward(self, images, captions):\n","        \"\"\"\n","        images: the input images, shape: [batch_size, 3, 224, 224]\n","        captions: the ground truth captions for teacher forcing, shape: [batch_size, max_caption_length]\n","        \"\"\"\n","        # TODO: Pasar las imágenes a través del encoder.\n","        encoder_out = self._encoder(images)\n","        # encoder_out: # [batch_size, 196, 512]\n","\n","        # Se definen el estado oculto y el estado de celda iniciales.\n","        batch_size = images.size(0)\n","        h, c = torch.zeros(batch_size, self._decoder_dim).to(self._device), torch.zeros(batch_size, self._decoder_dim).to(self._device)\n","        # h: [batch_size, decoder_dim]\n","        # c: [batch_size, decoder_dim]\n","\n","        # Placeholder para las palabras generadas.\n","        predictions = torch.zeros(batch_size, captions.size(1), self._vocab_size).to(self._device)\n","        # predictions: [batch_size, max_caption_length, vocab_size]\n","\n","        # Se establece la posición 0 de las predicciones para que sea la representación \"one-hot\" de <SOS>.\n","        predictions[:, 0, self._vocab[\"<SOS>\"]] = 1.0\n","\n","        # La primera palabra en el decoder será <SOS>.\n","        word = torch.tensor([self._vocab[\"<SOS>\"]] * batch_size).to(self._device)\n","        # word: [batch_size,]\n","\n","        for t in range(1, captions.size(1)):  # Empezamos en 1 ya que la primera palabra es <SOS>.\n","            embeddings = self._embedding(word)  # [batch_size, embed_dim]\n","            # TODO: Calcula el vector de contexto usando el módulo de atención.\n","            context_vector, _ = self._attention(h, encoder_out)\n","            # TODO: Ejecuta la propagación hacia adelante del decodificador.\n","            logits, h, c = self._decoder(context_vector, h, c, embeddings)\n","            predictions[:, t, :] = logits\n","            # Usa \"teacher forcing\": la siguiente entrada es el objetivo actual.\n","            word = captions[:, t]\n","\n","        return predictions"],"metadata":{"id":"vava5MFIoYh7"},"id":"vava5MFIoYh7","execution_count":null,"outputs":[]},{"cell_type":"code","source":["def test_sequence_to_sequence():\n","    # Parámetros\n","    embed_dim = 256\n","    decoder_dim = 512\n","    vocab_size = 5000  # Suponiendo un tamaño de vocabulario de 5000 palabras\n","    encoder_dim = 512\n","    attention_dim = 256\n","    device = \"cpu\"\n","    vocab = {\"<SOS>\": 1, \"<EOS>\": 2}  # Vocabulario de prueba temporal\n","\n","    # Inicializar el modelo\n","    model = SequenceToSequence(embed_dim, decoder_dim, vocab_size, encoder_dim, attention_dim, device, vocab)\n","\n","    # Datos de entrada\n","    images = torch.rand((8, 3, 224, 224))  # 8 imágenes\n","    captions = torch.randint(0, vocab_size, (8, 10))  # 8 descripciones con 10 palabras cada una\n","\n","    # Pasar los datos a través del modelo\n","    predictions = model(images, captions)\n","\n","    # Test 1: Verificar las dimensiones de salida\n","    assert predictions.shape == (8, 10, vocab_size), f\"Expected output shape (8, 10, {vocab_size}), but got {predictions.shape}\"\n","\n","    # Test 2: Verificar que la primera palabra de las predicciones es el token <SOS>\n","    sos_scores = predictions[:, 0, vocab[\"<SOS>\"]]\n","    assert torch.all(sos_scores == 1.0), \"¡La primera palabra de las predicciones no es <SOS>!\"\n","\n","    print(\"¡Todas las pruebas pasaron con éxito!\")\n","\n","test_sequence_to_sequence()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Fg0QkOV1osMX","executionInfo":{"status":"ok","timestamp":1755884120104,"user_tz":360,"elapsed":8584,"user":{"displayName":"Phabel López","userId":"10068691291319952559"}},"outputId":"bff1d8b6-6694-4bb4-c46c-6888f602e66e"},"id":"Fg0QkOV1osMX","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["¡Todas las pruebas pasaron con éxito!\n"]}]},{"cell_type":"markdown","id":"bc4cd8bb-bf85-4414-99e1-bb9cd607ed40","metadata":{"id":"bc4cd8bb-bf85-4414-99e1-bb9cd607ed40"},"source":["# 3. Creación de Datasets y Dataloaders"]},{"cell_type":"markdown","id":"84212edf-18ac-4b8e-acf5-94f98a2a07a8","metadata":{"id":"84212edf-18ac-4b8e-acf5-94f98a2a07a8"},"source":["En esta sección del cuaderno, vas a construir los \"Datasets\" y \"Dataloaders\" para entrenar el modelo. El primer paso para lograr esto es descargar y poner en la raíz de tu espacio de trabajo el conjunto de datos Flickr8K, esto lo puedes hacer dando click [aquí](https://drive.google.com/drive/folders/1CWkOJMddwuk7vqubQagIiCR9S09ceurX?usp=sharing). La estructura del conjunto de datos es la siguiente:\n","\n","- flickr8k/\n","    - Images/\n","        - image1.jpg\n","        - image2.jpg\n","        - ...\n","    - captions.txt\n","\n","`captions.txt` es un archivo de texto que contiene las descripciones de las imágenes en el formato: `{nombre_de_imagen},{descripción_de_imagen}`, la descripción de la imagen contiene cada token separado por un espacio lo que facilita su procesamiento. Cada imagen tiene cinco descripciones y se usarán todas para el entrenamiento del modelo.\n","\n","Al igual que en la sección pasada, en esta se requiere completar algunos fragmentos de código los cuales estarán indicados con la leyenda `TODO`. De igual manera, te puedes guíar en las pruebas unitarias para verificar el código que has escrito; sin embargo, estas no son exhaustivas y pasarlas no implica que tu código no tenga fallas o desperfectos en alguna parte."]},{"cell_type":"markdown","id":"6956ef37-04da-413e-a813-7a3c3d7fbe87","metadata":{"id":"6956ef37-04da-413e-a813-7a3c3d7fbe87"},"source":["## 3.1 Lectura y procesamiento de descripciones de imágenes."]},{"cell_type":"markdown","id":"1fd9b62a-64df-455c-a236-0b17b7694e3d","metadata":{"id":"1fd9b62a-64df-455c-a236-0b17b7694e3d"},"source":["El primer paso consiste en leer las descripciones de las imagenes. En esta sección no tienes que completar ninguna parte del código."]},{"cell_type":"code","execution_count":null,"id":"9a4e18f2-8244-452e-8766-9d261c4691f5","metadata":{"id":"9a4e18f2-8244-452e-8766-9d261c4691f5"},"outputs":[],"source":["def read_captions(filename: str) -> Dict[str, List[str]]:\n","    \"\"\"\n","    Lee las leyendas del conjunto de datos y crea un diccionario de  correspondencia de nombres de archivo\n","    de imágenes a una lista de descripciones.\n","\n","    :param filename: El nombre del archivo que contiene las descripciones.\n","    :return: Una correspondencia entre cada nombre de archivo de imagen en el conjunto de datos y sus\n","        descripciones correspondientes.\n","    \"\"\"\n","    with open(filename, 'rt') as f:\n","        lines: List[str] = f.readlines()\n","\n","    image_to_captions = defaultdict(list)\n","    for line in lines:\n","        image_name, caption = line.strip().split(',', 1)  # Dividir solamente en la primera aparición del símbolo ','\n","        image_to_captions[image_name].append(caption.strip().strip('\"').lower())\n","    return image_to_captions"]},{"cell_type":"code","execution_count":null,"id":"d38262fe-8063-4b42-98ed-640f03bc7d1b","metadata":{"id":"d38262fe-8063-4b42-98ed-640f03bc7d1b","outputId":"cc810fec-7a36-45f2-fb20-6389816209f3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1755884120112,"user_tz":360,"elapsed":7,"user":{"displayName":"Phabel López","userId":"10068691291319952559"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["caption_words[:20]: ['a', 'child', 'in', 'a', 'pink', 'dress', 'is', 'climbing', 'up', 'a', 'set', 'of', 'stairs', 'in', 'an', 'entry', 'way', '.', 'a', 'girl']\n"]}],"source":["captions = read_captions(\"/content/drive/MyDrive/GraduateCourseAI/NeuralNetworksSequences/captions.txt\")  # Recuerda poner el conjunto de datos en la raíz de tu espacio de trabajo.\n","caption_values: List[List[str]] = list(captions.values())\n","caption_words: List[str] = [word for caption_list in caption_values for caption in caption_list for word in caption.split()]\n","print(f\"caption_words[:20]: {caption_words[:20]}\")"]},{"cell_type":"markdown","id":"30dc9b6b-ff3a-42ef-8ab0-b5cdace712d3","metadata":{"id":"30dc9b6b-ff3a-42ef-8ab0-b5cdace712d3"},"source":["## 3.2 Generación del vocabulario."]},{"cell_type":"markdown","id":"d6fe8363-5916-4d89-9568-3154d2df6773","metadata":{"id":"d6fe8363-5916-4d89-9568-3154d2df6773"},"source":["En esta sección se define el vocabulario a usar utilizando la librería torchvision; sin embargo, tenemos que decidir qué palabras queremos incluir en el vocabulario. Una técnica común para tomar esta decisión es establecer un umbral de frecuencia,i.e., sólo las palabras que aparecen en el corpus más veces que este umbral se incluyen en el vocabulario. En estos casos es bastante útil construir una gráfica CCDF de las frecuencias de las palabras. Puedes leer más acerca de está gráfica [aquí](https://en.wikipedia.org/wiki/Cumulative_distribution_function#Complementary_cumulative_distribution_function_(tail_distribution)).\n","\n","La gráfica CCDF que aquí se presenta muestra la proporción de palabras en el corpus que tienen una frecuencia mayor o igual a un umbral específico. En otras palabras, nos dice qué porcentaje del vocabulario total se conservaría si estableciéramos diferentes umbrales de frecuencia.\n","\n","Seleccionar un valor de umbral es una decisión importante y que conlleva ciertas ventajas y desventajas, si seleccionamos un umbral bajo por ejemplo un valor de $1$ o $2$ tendríamos la ventaja de tener una mayor diversidad de palabras, lo que podría ser útil para capturar el significado en contextos específicos; sin embargo, también tiene el problema que aumenta el tamaño de memoria que necesitamos para entrenar el modelo y las palabras incluidas podrían no aportar mucho al entrenamiento y aumentar innecesariamente el tamaño del vocabulario.\n","\n","Por otro lado, si utilizamos un valor de umbral muy alto como 10,15 o 20, el vocabulario será más pequeño y estará compuesto por palabras comunes, lo que podría acelerar el entrenamiento del model; sin embargo, se pierde la dversidad en el vocabulario y por lo tanto su expresividad. Durante este ejercicio se ha seleccionado un valor de umbral de cinco la cual busca un equilibrio entre mantener un vocabulario diverso y evitar palabras demasiado raras o errores en el corpus. Durante esta sección no tienes que completar fragmentos de código."]},{"cell_type":"code","execution_count":null,"id":"4b3476c2-4c2a-4834-8abf-816fedb1e389","metadata":{"id":"4b3476c2-4c2a-4834-8abf-816fedb1e389","outputId":"57a278e6-7872-4412-cb50-c7802634e6a0","colab":{"base_uri":"https://localhost:8080/","height":772},"executionInfo":{"status":"ok","timestamp":1755884120671,"user_tz":360,"elapsed":558,"user":{"displayName":"Phabel López","userId":"10068691291319952559"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Con un umbral de frecuencia de 1, la proporción de palabras incluidas es 1.0 o 9628 palabras.\n","Con un umbral de frecuencia de 2, la proporción de palabras incluidas es 0.5872588604755495 o 5654 palabras.\n","Con un umbral de frecuencia de 3, la proporción de palabras incluidas es 0.4600717810677434 o 4429 palabras.\n","Con un umbral de frecuencia de 4, la proporción de palabras incluidas es 0.3857110812023329 o 3713 palabras.\n","Con un umbral de frecuencia de 5, la proporción de palabras incluidas es 0.3364737550471063 o 3239 palabras.\n","Con un umbral de frecuencia de 6, la proporción de palabras incluidas es 0.297442799461642 o 2863 palabras.\n","Con un umbral de frecuencia de 7, la proporción de palabras incluidas es 0.2731045311799013 o 2629 palabras.\n","Con un umbral de frecuencia de 8, la proporción de palabras incluidas es 0.2538133692238672 o 2443 palabras.\n","Con un umbral de frecuencia de 9, la proporción de palabras incluidas es 0.23788694481830416 o 2290 palabras.\n","Con un umbral de frecuencia de 10, la proporción de palabras incluidas es 0.22016599371915657 o 2119 palabras.\n","Con un umbral de frecuencia de 11, la proporción de palabras incluidas es 0.2079407806191117 o 2002 palabras.\n","Con un umbral de frecuencia de 12, la proporción de palabras incluidas es 0.199304620906236 o 1918 palabras.\n","Con un umbral de frecuencia de 13, la proporción de palabras incluidas es 0.19055630327501122 o 1834 palabras.\n","Con un umbral de frecuencia de 14, la proporción de palabras incluidas es 0.18281740690892778 o 1760 palabras.\n","Con un umbral de frecuencia de 15, la proporción de palabras incluidas es 0.1753028263795424 o 1687 palabras.\n","Con un umbral de frecuencia de 16, la proporción de palabras incluidas es 0.16733961417676088 o 1611 palabras.\n","Con un umbral de frecuencia de 17, la proporción de palabras incluidas es 0.16094661283086587 o 1549 palabras.\n","Con un umbral de frecuencia de 18, la proporción de palabras incluidas es 0.15489008524001793 o 1491 palabras.\n","Con un umbral de frecuencia de 19, la proporción de palabras incluidas es 0.14950650515926425 o 1439 palabras.\n","Con un umbral de frecuencia de 20, la proporción de palabras incluidas es 0.14513234634365185 o 1397 palabras.\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 800x400 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}],"source":["# Se obtienen las frecuencias de palabras y el número total de palabras.\n","freqs = Counter(caption_words)\n","frequencies = list(freqs.values())\n","total_words = len(frequencies)\n","\n","# Se calculan los valores de CCDF para cada umbral de frecuencia de 0 a 20.\n","thresholds = list(range(1, 21))\n","proportions = [sum(np.array(frequencies) >= threshold) / total_words for threshold in thresholds]\n","\n","for p, t in zip(proportions, thresholds):\n","    print(f\"Con un umbral de frecuencia de {t}, la proporción de palabras incluidas es {p} o {int(p * 9_628)} palabras.\")\n","\n","# Usamos matplolib para crear la gráfica CCDF.\n","plt.figure(figsize=(8, 4))\n","plt.plot(thresholds, proportions, marker='o')\n","plt.xlabel(\"Frequency Threshold\")\n","plt.ylabel(\"Proportion of Words\")\n","plt.title(\"CCDF of Word Frequencies\")\n","plt.xticks(thresholds)\n","plt.grid()\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"id":"a474d680-14cc-4db3-a8f3-0899f4497705","metadata":{"id":"a474d680-14cc-4db3-a8f3-0899f4497705"},"outputs":[],"source":["# Definimos nuestro vocabulario usando torchtext, en específico usamos la función `build_vocab_from_iterator`\n","# Puedes leer más acerca de esta función aquí https://pytorch.org/text/stable/vocab.html#build-vocab-from-iterator.\n","vocabulary = torchtext.vocab.build_vocab_from_iterator(\n","    iterator=[caption_words],  # torchtext necesita que el iterados sea de tipo List[List[str]]\n","    min_freq=5,  # Frecuencia mínima para aceptar una palabra en el corpus\n","    specials=[\"<PAD>\", \"<SOS>\", \"<EOS>\", \"<UNK>\"],\n","    special_first=True  # Define que las palabras especiales tomarán los primeros índices del vocabulario.\n",")\n","# Valor del índice predeterminado. Este índice se devolverá cuando se consulte un token OOV (Out of Vocabulary).\n","vocabulary.set_default_index(index=3)"]},{"cell_type":"code","execution_count":null,"id":"f8688677-2635-47dc-a97e-a3fd1cf7ea79","metadata":{"id":"f8688677-2635-47dc-a97e-a3fd1cf7ea79","outputId":"c4886119-d1ce-48a7-8f3d-00f1c755371d","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1755884120679,"user_tz":360,"elapsed":6,"user":{"displayName":"Phabel López","userId":"10068691291319952559"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(3004, 3)"]},"metadata":{},"execution_count":18}],"source":["len(vocabulary), vocabulary[\"non_existing_word\"]"]},{"cell_type":"markdown","id":"8ff5dbb7-ecae-40e2-95a4-d0cdc27690ad","metadata":{"id":"8ff5dbb7-ecae-40e2-95a4-d0cdc27690ad"},"source":["## 3.3 Definición del Dataset \"Flickr8kDataset\" en PyTorch."]},{"cell_type":"markdown","id":"c866c0ce-b1cf-4a98-b256-3b7146b374a0","metadata":{"id":"c866c0ce-b1cf-4a98-b256-3b7146b374a0"},"source":["Esta sección del cuaderno se centra en la preparación y estructuración del conjunto de datos, para esto, definiremos este como una clase que hereda de `torch.utils.data.Dataset` encargada de regresar una tupla de Tensores representando una imagen y su descripción. De manera específica, la clase será la encargada de:\n","\n","- Cargar las imágenes y sus descripciones correspondientes.\n","- Aplicar transformaciones a las imágenes para que estén en un formato adecuado para nuestro modelo.\n","- Tokenizar las descripciones para convertirlas en secuencias de números enteros que representan palabras.\n","\n","En esta sección hay partes del código que se han omitido intencionadamente. Tu tarea será completar estas secciones, asegurándote de que el conjunto de datos pueda ser utilizado correctamente por el modelo que entrenaremos más adelante."]},{"cell_type":"code","source":["class Flickr8kDataset(torch.utils.data.Dataset):\n","    \"\"\"Define the dataset class. The dataset was downloaded from https://www.kaggle.com/datasets/adityajn105/flickr8k\"\"\"\n","    def __init__(self, image_directory: str, image_to_captions_mapping: Dict[str, List[str]], vocabulary, transform=None) -> None:\n","        self._image_directory = image_directory\n","        self._image_to_captions_mapping = image_to_captions_mapping\n","        self._vocabulary = vocabulary\n","        self._transform = transform\n","\n","        # TODO: Aplana la estructura de `image_to_captions_mapping`: cada nombre de imagen se repetirá\n","        # según el número de sus leyendas. Como ayuda se define el tipo de dato de _image_caption_pairs\n","        # la cual es una lista de tuplas en donde cada tupla contiene el nombre de la imagen y su descripción.\n","        self._image_caption_pairs: List[Tuple[str, str]] = []\n","\n","        # Flatten the mapping\n","        for image_name, captions_list in image_to_captions_mapping.items():\n","            for caption in captions_list:\n","                self._image_caption_pairs.append((image_name, caption))\n","\n","    def __len__(self) -> int:\n","        # TODO: Regresa el tamaño correcto del conjunto de datos.\n","        return len(self._image_caption_pairs)\n","\n","    def __getitem__(self, idx: int):\n","        image_name, caption = self._image_caption_pairs[idx]\n","        image_path = os.path.join(self._image_directory, image_name)\n","        image = Image.open(image_path).convert('RGB')\n","\n","        # TODO: Aplica las transformaciones a la imagen si estas son proporcionadas.\n","        if self._transform is not None:\n","            image = self._transform(image)\n","\n","        tokenized_caption = [self._vocabulary[word] for word in caption.split(\" \")]\n","        caption = [self._vocabulary[\"<SOS>\"]] + tokenized_caption + [self._vocabulary[\"<EOS>\"]]\n","        return image, torch.tensor(caption)"],"metadata":{"id":"XceKb93D8a_U"},"id":"XceKb93D8a_U","execution_count":null,"outputs":[]},{"cell_type":"code","source":["def caption_collate_fn(batch: List[Tuple[torch.Tensor, torch.Tensor]]) -> Tuple[torch.Tensor, torch.Tensor]:\n","    \"\"\"Función para agregar padding a las descripciones de imágenes por bache.\n","\n","    :param batch: Una lista de tuplas donde cada tupla contiene una imagen y su descripción.\n","    :return: Una tupla que contiene dos tensores:\n","             1) Un tensor que contiene todas las imágenes del lote, apiladas juntas. Forma: [batch_size, 3, altura, ancho].\n","             2) Un tensor que contiene todas las descripciones del bache rellenadas con zero para que tengan la misma longitud.\n","                Forma: [batch_size, longitud_maxima_secuencia].\n","    \"\"\"\n","    images = [item[0] for item in batch]\n","    captions = [item[1] for item in batch]\n","\n","    images = torch.stack(images, dim=0)\n","    # images: [batch_size, 3, height, width]\n","\n","    # Agregar padding a las secuencias.\n","    lengths = [len(cap) for cap in captions]\n","    targets = torch.zeros(len(captions), max(lengths)).long()\n","    # Recuerda que el índice del token <PAD> es 0.\n","    # targets: [batch_size, max_sequence_length]\n","\n","    for idx, cap in enumerate(captions):\n","        end = lengths[idx]\n","        targets[idx, :end] = cap[:end]\n","\n","    return images, targets"],"metadata":{"id":"ns_RQqBxKmow"},"id":"ns_RQqBxKmow","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Definir transformaciones (para VGG16)\n","transform = transforms.Compose([\n","    transforms.Resize((224, 224)),          # 224x224 para imágenes\n","    transforms.ToTensor(),                  # Convertir PIL Image a tensor\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n","                        std=[0.229, 0.224, 0.225])  # ImageNet normalización\n","])\n","\n","# Crear dataset con los parámetros requeridos\n","dataset = Flickr8kDataset(\n","    image_directory=\"/content/drive/MyDrive/GraduateCourseAI/NeuralNetworksSequences/Images\",\n","    image_to_captions_mapping=captions,\n","    vocabulary=vocabulary,\n","    transform=transform\n",")\n","\n","print(f\"Total dataset size: {len(dataset)} image-caption pairs\")\n","\n","# Separar dataset\n","train_size = int(0.9 * len(dataset))\n","test_size = len(dataset) - train_size\n","train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n","\n","print(f\"Training size: {len(train_dataset)}\")\n","print(f\"Test size: {len(test_dataset)}\")\n","\n","# Crear dataloaders\n","batch_size = 32\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0, collate_fn=caption_collate_fn)\n","test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0, collate_fn=caption_collate_fn)\n","\n","print(\"Dataset and DataLoaders created successfully!\")\n","\n","# Test parcial\n","print(\"\\n=== Testing first batch ===\")\n","for i, (images, captions) in enumerate(train_loader):\n","    print(f\"Images shape: {images.shape}\")\n","    print(f\"Captions shape: {captions.shape}\")\n","    print(\"First batch loaded successfully!\")\n","    break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"El3e_4MeT4TM","executionInfo":{"status":"ok","timestamp":1755884123153,"user_tz":360,"elapsed":2452,"user":{"displayName":"Phabel López","userId":"10068691291319952559"}},"outputId":"5cd516c9-7def-4efd-92e4-36ff9873e1b0"},"id":"El3e_4MeT4TM","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Total dataset size: 40455 image-caption pairs\n","Training size: 36409\n","Test size: 4046\n","Dataset and DataLoaders created successfully!\n","\n","=== Testing first batch ===\n","Images shape: torch.Size([32, 3, 224, 224])\n","Captions shape: torch.Size([32, 23])\n","First batch loaded successfully!\n"]}]},{"cell_type":"markdown","id":"bcf1b969-16e0-4736-b58d-caf689f98866","metadata":{"id":"bcf1b969-16e0-4736-b58d-caf689f98866"},"source":["# 3.4 División de conjuntos de datos y definición de dataloaders."]},{"cell_type":"markdown","id":"51c86fa9-c0bb-419b-b4d4-28c33d075f93","metadata":{"id":"51c86fa9-c0bb-419b-b4d4-28c33d075f93"},"source":["En esta sección del cuaderno de realizan dos cosas:\n","1. Se instancían los conjuntos de entrenamiento y de prueba utilizando la función `torch.utils.data.random_split`.\n","2. Se define una función para agregar padding a todas las oraciones con el objetivo homologar la lonngitud de estas para poder entrenar el modelo. Esta función será pasada como argumento en el parámerto `collate_fn` de los Dataloaders.\n","\n","En esta sección no es necesario que completes ningún fragmento de código."]},{"cell_type":"code","execution_count":null,"id":"74902235-034c-437e-b872-e130323d03b5","metadata":{"id":"74902235-034c-437e-b872-e130323d03b5","executionInfo":{"status":"ok","timestamp":1755884123169,"user_tz":360,"elapsed":15,"user":{"displayName":"Phabel López","userId":"10068691291319952559"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"900d6e22-2484-4618-eed0-7c6c317ed8d7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Training size: 36409\n","Test size: 4046\n"]}],"source":["# Separar dataset\n","train_size = int(0.9 * len(dataset))\n","test_size = len(dataset) - train_size\n","train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n","\n","print(f\"Training size: {len(train_dataset)}\")\n","print(f\"Test size: {len(test_dataset)}\")"]},{"cell_type":"code","execution_count":null,"id":"ad0d111e-1a5f-4043-a4fd-b76af186179f","metadata":{"id":"ad0d111e-1a5f-4043-a4fd-b76af186179f"},"outputs":[],"source":["def caption_collate_fn(batch: List[Tuple[torch.Tensor, torch.Tensor]]) -> Tuple[torch.Tensor, torch.Tensor]:\n","    \"\"\"Función para agregar padding a las descripciones de imágenes por bache.\n","\n","    :param batch: Una lista de tuplas donde cada tupla contiene una imagen y su descripción.\n","    :return: Una tupla que contiene dos tensores:\n","             1) Un tensor que contiene todas las imágenes del lote, apiladas juntas. Forma: [batch_size, 3, altura, ancho].\n","             2) Un tensor que contiene todas las descripciones del bache rellenadas con zero para que tengan la misma longitud.\n","                Forma: [batch_size, longitud_maxima_secuencia].\n","    \"\"\"\n","    images = [item[0] for item in batch]\n","    captions = [item[1] for item in batch]\n","\n","    images = torch.stack(images, dim=0)\n","    # images: [batch_size, 3, height, width]\n","\n","    # Agregar padding a las secuencias.\n","    lengths = [len(cap) for cap in captions]\n","    targets = torch.zeros(len(captions), max(lengths)).long()\n","    # Recuerda que el índice del token <PAD> es 0.\n","    # targets: [batch_size, max_sequence_length]\n","\n","    for idx, cap in enumerate(captions):\n","        end = lengths[idx]\n","        targets[idx, :end] = cap[:end]\n","\n","    return images, targets\n","\n","batch_size = 32\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0, collate_fn=caption_collate_fn)\n","test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0, collate_fn=caption_collate_fn)"]},{"cell_type":"markdown","id":"30c471b1-70f4-46f0-a3b7-5a98f8df1a23","metadata":{"id":"30c471b1-70f4-46f0-a3b7-5a98f8df1a23"},"source":["# 4. Entrenamiento y Evaluación del Modelo."]},{"cell_type":"markdown","id":"c064ca64-5c7d-4e6c-9e82-5a38bc3c9199","metadata":{"id":"c064ca64-5c7d-4e6c-9e82-5a38bc3c9199"},"source":["Tras haber definido y preparado nuestro conjunto de datos, es hora de diseñar el corazón del proceso: el entrenamiento y evaluación de nuestro modelo. En esta sección, implementarás los detalles técnicos que permitirán al modelo aprender a generar descripciones a partir de imágenes.\n","\n","Primero, se establecen varios parámetros esenciales que influirán en la capacidad y eficiencia de nuestro modelo, tales como embed_dim (tamaño de los embeddings), decoder_dim (estado oculto del decodificador), vocab_size (tamaño de nuestro vocabulario), encoder_dim (tamaño de los vectores de anotaciones) y attention_dim (tamaño de los tensores de pesos de las capas lineales del modelo de atención). Estos parámetros determinan la estructura y complejidad de nuestra red neuronal, y su elección adecuada es crucial para el rendimiento del modelo.\n","\n","A continuación, se instancien tres partes fundamentes para el entrenamiento:\n","- El modelo a entrenar.\n","- El optimizador a utlizar para actualizar los pesos.\n","- La función de pérdida para el cálculo de los gradientes.\n","\n","Una vez que todo esté en su lugar, diseñarás una función que se encargará de entrenar al modelo. Esta función iterará a través del conjunto de datos, alimentando imágenes al modelo, ajustando sus pesos en función de las predicciones y la realidad, y mejorando paso a paso su capacidad de generar descripciones. Es en esta parte donde tendrás que completar ciertos fragmentos de código.\n","\n","Finalmente, después de entrenar nuestro modelo, lo guardaremos en disco. Esto nos permitirá reutilizarlo en el futuro sin tener que volver a entrenarlo, y también compartirlo con otros o implementarlo en aplicaciones prácticas."]},{"cell_type":"code","execution_count":null,"id":"434a4988-d9ba-4121-b7da-652f93bd34e8","metadata":{"id":"434a4988-d9ba-4121-b7da-652f93bd34e8"},"outputs":[],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","embed_dim = 300\n","decoder_dim = 2048\n","vocab_size = len(vocabulary)\n","encoder_dim = 512\n","attention_dim = 2048\n","\n","model = SequenceToSequence(embed_dim, decoder_dim, vocab_size, encoder_dim, attention_dim, device, vocabulary).to(device)\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","criterion = torch.nn.CrossEntropyLoss(ignore_index=vocabulary[\"<pad>\"])  # Ignoramos el padding al calcular la pérdida."]},{"cell_type":"code","source":["def train_epoch(model, dataloader, optimizer, criterion, device):\n","    model.train()\n","    epoch_loss = 0\n","    all_generated_captions = []\n","    all_true_captions = []\n","\n","    for i, (images, captions) in enumerate(dataloader):\n","        # TODO: Mueve las imágenes y las descripciones al dispositivo adecuado (CPU o GPU).\n","        images, captions = images.to(device), captions.to(device)\n","\n","        optimizer.zero_grad()\n","\n","        # TODO: Pasa las imágenes y las descripciones al modelo para obtener las salidas.\n","        outputs = model(images, captions)\n","\n","        # Cambiar dimensiones para el cálculo de la función de pérdida.\n","        outputs = outputs.view(-1, outputs.size(2))\n","        captions = captions.view(-1)\n","\n","        # TODO: Calcula la función de pérdida.\n","        loss = criterion(outputs, captions)\n","        loss.backward()\n","        optimizer.step()\n","\n","        epoch_loss += loss.item()\n","\n","        # Generar descripciones para evaluar al modelo.\n","        top1 = outputs.argmax(1)\n","        generated_captions = [[vocabulary.lookup_token(word.item()) for word in desc] for desc in top1.chunk(batch_size)]\n","        true_captions = [[vocabulary.lookup_token(word.item()) for word in desc] for desc in captions.chunk(batch_size)]\n","\n","        if i % 50 == 0:\n","            print(f\"i: {i}, loss: {loss}, true: {true_captions[0]}, generated: {generated_captions[0]}\")\n","\n","    return epoch_loss / len(dataloader)"],"metadata":{"id":"WDS-7Soy8ziU"},"id":"WDS-7Soy8ziU","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Entrenamiento del modelo.\n","NUM_EPOCHS = 1\n","train_losses = []\n","for epoch in range(NUM_EPOCHS):\n","    train_loss = train_epoch(model, train_loader, optimizer, criterion, device)\n","    train_losses.append(train_loss)\n","    print(f\"Epoch: {epoch + 1}, train loss: {train_loss:.3f}\")"],"metadata":{"id":"fKzaXFCH5Jwb"},"id":"fKzaXFCH5Jwb","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Guardamos el modelo en disco.\n","#torch.save(model.cpu().state_dict(), '/content/drive/MyDrive/GraduateCourseAI/NeuralNetworksSequences/seq2seq_model.pth')"],"metadata":{"id":"njjCZuRu5GoB"},"id":"njjCZuRu5GoB","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"d17ae124-35ec-47dc-9d59-19f16233d7b0","metadata":{"id":"d17ae124-35ec-47dc-9d59-19f16233d7b0"},"source":["# 5. Análisis de Resutados, Generación de Descripciones"]},{"cell_type":"markdown","id":"965bc459-dd94-4583-b528-2e23509c2407","metadata":{"id":"965bc459-dd94-4583-b528-2e23509c2407"},"source":["¡Felicidades por haber llegado hasta este punto!\n","\n","Has trabajado en las secciones anteriores y has construido un modelo de descripción de imágenes desde cero. Ahora, en esta sección , vamos a ver cómo se desempeña el modelo en la generación de descripciones para imágenes. Entrenar el modelo puede tomar algo de tiempo; sin embargo, si tienes la oportunidad de hacerlo te lo recomiendo. De esta forma te darás cuenta del desempeño de esto y de que puntos se pueden realizar para mejorarlo. Mientras tanto, te dejo aquí algunos ejemplos de algunas descripciones que generé con un modelo ya entrenado."]},{"cell_type":"markdown","id":"993c6330-a316-4363-8319-cc0095e1bf3d","metadata":{"id":"993c6330-a316-4363-8319-cc0095e1bf3d"},"source":["## Imagen 1"]},{"cell_type":"markdown","id":"dd5a65c3-64ca-447a-ad26-f99c64a5b1aa","metadata":{"id":"dd5a65c3-64ca-447a-ad26-f99c64a5b1aa"},"source":["![2887171449_f54a2b9f39.jpg](attachment:1d467e20-2525-48ce-8833-c2993d79ac4c.jpg)"]},{"cell_type":"markdown","id":"814053af-4a3e-4fbe-9daf-b8dfb5674642","metadata":{"id":"814053af-4a3e-4fbe-9daf-b8dfb5674642"},"source":["**Descripción original**: A woman with glasses is sewing.\n","\n","**Descripción generada**: A man with a a sewing machine"]},{"cell_type":"markdown","id":"c7e61538-7053-43e3-885a-5cdc65755bb8","metadata":{"id":"c7e61538-7053-43e3-885a-5cdc65755bb8"},"source":["## Imagen 2"]},{"cell_type":"markdown","id":"d440dfa4-7f6f-4f9e-b715-99bc58da2722","metadata":{"id":"d440dfa4-7f6f-4f9e-b715-99bc58da2722"},"source":["![3514179514_cbc3371b92.jpg](attachment:b2e09945-1f00-40b5-abfd-a4d56f94c704.jpg)"]},{"cell_type":"markdown","id":"4dedd889-c0ef-4785-93f6-fda057b282d9","metadata":{"id":"4dedd889-c0ef-4785-93f6-fda057b282d9"},"source":["**Descripción original**: A baseball player attempts to catch a ball while another runs towards the base.\n","\n","**Descripción generada**: A baseman player swings to a a base. One. for. base behind."]},{"cell_type":"markdown","id":"e832ee8a-0a48-40aa-8ee7-6b0c9b021b89","metadata":{"id":"e832ee8a-0a48-40aa-8ee7-6b0c9b021b89"},"source":["## Imagen 3"]},{"cell_type":"markdown","id":"88e97be0-3caf-4f2a-ac4e-49b1627e8132","metadata":{"id":"88e97be0-3caf-4f2a-ac4e-49b1627e8132"},"source":["![2981702521_2459f2c1c4.jpg](attachment:e4f738cd-ead6-4d6b-8bb5-91a4cd63c952.jpg)"]},{"cell_type":"markdown","id":"54e87e4c-536d-41ef-badb-897f65a79345","metadata":{"id":"54e87e4c-536d-41ef-badb-897f65a79345"},"source":["**Descripción original**: People standing in front of a subway train.\n","\n","**Descripción generada**: The wait in the of a subway station."]},{"cell_type":"markdown","id":"753fc188-7b2b-4b8f-8074-47197659669b","metadata":{"id":"753fc188-7b2b-4b8f-8074-47197659669b"},"source":["# Imagen 4"]},{"cell_type":"markdown","id":"5f1236b1-2ae9-4356-b670-4b83ba7feb4b","metadata":{"id":"5f1236b1-2ae9-4356-b670-4b83ba7feb4b"},"source":["![2727051596_be65bfb3d3.jpg](attachment:46b5ed67-9f12-4431-a769-fe93a57939ed.jpg)"]},{"cell_type":"markdown","id":"834b7338-150f-440d-a40c-371ead36dc3d","metadata":{"id":"834b7338-150f-440d-a40c-371ead36dc3d"},"source":["**Descripción original**: Two boys enjoy a ride at an amusement park .\n","\n","**Descripción generada**: Two boys enjoy a ride in lying amusement park. ride"]},{"cell_type":"markdown","id":"da19b0f1-2ffc-4344-8e5b-00b3fc45dc2a","metadata":{"id":"da19b0f1-2ffc-4344-8e5b-00b3fc45dc2a"},"source":["Como puedes ver, el modelo a pesar de tener una idea general sobre lo que está ocurriendo en las imágenes, todavía tiene muchos puntos en los que puede mejorar. Sin embargo, no hay que desanimarnos por esto, hay varios puntos de mejora que puedes considerar para mejorar el rendimiento del modelo:\n","\n","- Implementar la función de pérdida usada en el artículo \"Doubly Stochastic Attention\": Esta función de pérdida puede ayudar a mejorar la atención del modelo y, por lo tanto, a generar descripciones más precisas.\n","- Utilizar más datos de entrenamiento: Como en cualquier modelo de aprendizaje automático, más datos pueden llevar a un mejor rendimiento. Considera ampliar tu conjunto de datos o utilizar técnicas de aumento de datos.\n","- Utilizar un decodificador de múltiples capas en lugar de una: Un decodificador más complejo puede capturar relaciones más intrincadas en los datos y mejorar la calidad de las descripciones generadas.\n","\n","Con estos puntos en mente, te recomiendo seguir explorando con el modelo y ver que tan precisas son las descripciones que puedes generar. ¡Buena suerte!"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}